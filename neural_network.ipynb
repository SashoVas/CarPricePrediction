{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de3a312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression,ElasticNet,Lasso,Ridge\n",
    "from visualization_utils import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score,root_mean_squared_error,mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c915dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_MODELS = False\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72998330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict, Optional\n",
    "\n",
    "# --- Mapping dictionaries (same categories we discussed) ---\n",
    "brand_price_tier: Dict[str, str] = {\n",
    "    \"audi\": \"premium\", \"porsche\": \"luxury\", \"bmw\": \"luxury\", \"dodge\": \"mainstream\",\n",
    "    \"dongfeng\": \"value\", \"ford\": \"mainstream\", \"honda\": \"mainstream\", \"hyundai\": \"mainstream\",\n",
    "    \"land\": \"premium\", \"lexus\": \"premium\", \"maserati\": \"luxury\", \"mercedes-benz\": \"luxury\",\n",
    "    \"skoda\": \"mainstream\", \"subaru\": \"mainstream\", \"toyota\": \"mainstream\", \"vw\": \"mainstream\",\n",
    "    \"volvo\": \"premium\", \"alfa\": \"premium\", \"cupra\": \"upper_mid\", \"ds\": \"upper_mid\",\n",
    "    \"haval\": \"value\", \"jeep\": \"upper_mid\", \"kia\": \"mainstream\", \"mazda\": \"mainstream\",\n",
    "    \"mini\": \"premium\", \"peugeot\": \"mainstream\", \"mitsubishi\": \"mainstream\", \"nissan\": \"mainstream\",\n",
    "    \"seat\": \"mainstream\", \"ssangyong\": \"value\", \"tesla\": \"premium\", \"acura\": \"premium\",\n",
    "    \"alpina\": \"premium\", \"aston\": \"ultra_luxury\", \"bentley\": \"ultra_luxury\", \"byd\": \"value\",\n",
    "    \"cadillac\": \"premium\", \"chevrolet\": \"mainstream\", \"chrysler\": \"mainstream\", \"citroen\": \"mainstream\",\n",
    "    \"fiat\": \"mainstream\", \"foton\": \"value\", \"geely\": \"value\", \"genesis\": \"premium\", \"gmc\": \"premium\",\n",
    "    \"great\": \"value\", \"jaguar\": \"premium\", \"infiniti\": \"premium\", \"lincoln\": \"premium\",\n",
    "    \"lynkco\": \"upper_mid\", \"mg\": \"value\", \"opel\": \"mainstream\", \"renault\": \"mainstream\",\n",
    "    \"smart\": \"niche\", \"voyah\": \"value\", \"dacia\": \"budget\", \"lada\": \"budget\", \"pontiac\": \"defunct\",\n",
    "    \"suzuki\": \"mainstream\", \"daihatsu\": \"value\", \"gaz\": \"value\", \"isuzu\": \"value\", \"lancia\": \"niche\",\n",
    "    \"mahindra\": \"value\", \"saab\": \"defunct\", \"daewoo\": \"defunct\", \"moskvich\": \"defunct\",\n",
    "    \"trabant\": \"defunct\", \"triumph\": \"defunct\", \"uaz\": \"value\", \"wartburg\": \"defunct\",\n",
    "    \"lamborghini\": \"ultra_luxury\", \"ferrari\": \"ultra_luxury\", \"maybach\": \"ultra_luxury\",\n",
    "    \"baic\": \"value\", \"ineos\": \"niche\", \"morgan\": \"niche\", \"polestar\": \"premium\", \"tata\": \"value\",\n",
    "    \"volga\": \"defunct\", \"abarth\": \"upper_mid\", \"hummer\": \"niche\", \"iveco\": \"value\",\n",
    "    \"mclaren\": \"ultra_luxury\", \"rolls-royce\": \"ultra_luxury\", \"dkw\": \"defunct\", \"lotus\": \"niche\",\n",
    "    \"secma\": \"niche\", \"други\": \"unknown\", \"baw\": \"value\", \"carbodies\": \"niche\", \"rover\": \"defunct\",\n",
    "    \"talbot\": \"defunct\", \"daimler\": \"niche\", \"dr\": \"niche\", \"oldsmobile\": \"defunct\", \"buick\": \"premium\",\n",
    "    \"gwm\": \"value\", \"swm\": \"value\", \"shuanghuan\": \"value\", \"changan\": \"value\", \"leapmotor\": \"value\",\n",
    "    \"dfsk\": \"value\", \"чайка\": \"defunct\", \"austin\": \"defunct\", \"tempo\": \"defunct\", \"zaz\": \"defunct\",\n",
    "    \"seres\": \"value\", \"maxus\": \"value\", \"microcar\": \"niche\", \"aixam\": \"niche\", \"chery\": \"value\",\n",
    "    \"fisker\": \"niche\", \"jac\": \"value\", \"landwind\": \"value\", \"eagle\": \"defunct\", \"goupil\": \"niche\",\n",
    "    \"brilliance\": \"value\", \"plymouth\": \"defunct\", \"asia\": \"unknown\", \"gonow\": \"value\", \"simca\": \"defunct\",\n",
    "    \"wey\": \"upper_mid\", \"hongqi\": \"upper_mid\"\n",
    "}\n",
    "\n",
    "brand_region: Dict[str, str] = {\n",
    "    \"audi\": \"europe\", \"porsche\": \"europe\", \"bmw\": \"europe\", \"dodge\": \"north_america\", \"dongfeng\": \"china\",\n",
    "    \"ford\": \"north_america\", \"honda\": \"asia\", \"hyundai\": \"south_korea\", \"land\": \"uk\", \"lexus\": \"asia\",\n",
    "    \"maserati\": \"europe\", \"mercedes-benz\": \"europe\", \"skoda\": \"europe\", \"subaru\": \"asia\",\n",
    "    \"toyota\": \"asia\", \"vw\": \"europe\", \"volvo\": \"europe\", \"alfa\": \"europe\", \"cupra\": \"europe\",\n",
    "    \"ds\": \"europe\", \"haval\": \"china\", \"jeep\": \"north_america\", \"kia\": \"south_korea\", \"mazda\": \"asia\",\n",
    "    \"mini\": \"uk\", \"peugeot\": \"europe\", \"mitsubishi\": \"asia\", \"nissan\": \"asia\", \"seat\": \"europe\",\n",
    "    \"ssangyong\": \"south_korea\", \"tesla\": \"north_america\", \"acura\": \"asia\", \"alpina\": \"europe\",\n",
    "    \"aston\": \"uk\", \"bentley\": \"uk\", \"byd\": \"china\", \"cadillac\": \"north_america\", \"chevrolet\": \"north_america\",\n",
    "    \"chrysler\": \"north_america\", \"citroen\": \"europe\", \"fiat\": \"europe\", \"foton\": \"china\", \"geely\": \"china\",\n",
    "    \"genesis\": \"south_korea\", \"gmc\": \"north_america\", \"great\": \"china\", \"jaguar\": \"uk\", \"infiniti\": \"asia\",\n",
    "    \"lincoln\": \"north_america\", \"lynkco\": \"china\", \"mg\": \"china\", \"opel\": \"europe\", \"renault\": \"europe\",\n",
    "    \"smart\": \"europe\", \"voyah\": \"china\", \"dacia\": \"europe\", \"lada\": \"russia\", \"pontiac\": \"north_america\",\n",
    "    \"suzuki\": \"asia\", \"daihatsu\": \"asia\", \"gaz\": \"russia\", \"isuzu\": \"asia\", \"lancia\": \"europe\",\n",
    "    \"mahindra\": \"india\", \"saab\": \"europe\", \"daewoo\": \"asia\", \"moskvich\": \"russia\", \"trabant\": \"europe\",\n",
    "    \"triumph\": \"uk\", \"uaz\": \"russia\", \"wartburg\": \"europe\", \"lamborghini\": \"europe\", \"ferrari\": \"europe\",\n",
    "    \"maybach\": \"europe\", \"baic\": \"china\", \"ineos\": \"europe\", \"morgan\": \"uk\", \"polestar\": \"europe\",\n",
    "    \"tata\": \"india\", \"volga\": \"russia\", \"abarth\": \"europe\", \"hummer\": \"north_america\", \"iveco\": \"europe\",\n",
    "    \"mclaren\": \"uk\", \"rolls-royce\": \"uk\", \"dkw\": \"europe\", \"lotus\": \"uk\", \"secma\": \"europe\", \"други\": \"unknown\",\n",
    "    \"baw\": \"china\", \"carbodies\": \"uk\", \"rover\": \"uk\", \"talbot\": \"europe\", \"daimler\": \"europe\", \"dr\": \"europe\",\n",
    "    \"oldsmobile\": \"north_america\", \"buick\": \"north_america\", \"gwm\": \"china\", \"swm\": \"china\", \"shuanghuan\": \"china\",\n",
    "    \"changan\": \"china\", \"leapmotor\": \"china\", \"dfsk\": \"china\", \"чайка\": \"russia\", \"austin\": \"uk\", \"tempo\": \"europe\",\n",
    "    \"zaz\": \"europe\", \"seres\": \"china\", \"maxus\": \"china\", \"microcar\": \"europe\", \"aixam\": \"europe\", \"chery\": \"china\",\n",
    "    \"fisker\": \"north_america\", \"jac\": \"china\", \"landwind\": \"china\", \"eagle\": \"north_america\", \"goupil\": \"europe\",\n",
    "    \"brilliance\": \"china\", \"plymouth\": \"north_america\", \"asia\": \"unknown\", \"gonow\": \"china\", \"simca\": \"europe\",\n",
    "    \"wey\": \"china\", \"hongqi\": \"china\"\n",
    "}\n",
    "\n",
    "brand_ev_focus: Dict[str, str] = {\n",
    "    \"tesla\": \"ev_leader\", \"byd\": \"ev_leader\", \"polestar\": \"ev_active\", \"fisker\": \"ev_active\",\n",
    "    \"nissan\": \"ev_present\", \"bmw\": \"ev_present\", \"audi\": \"ev_present\", \"mercedes-benz\": \"ev_present\",\n",
    "    \"vw\": \"ev_present\", \"hyundai\": \"ev_present\", \"kia\": \"ev_present\", \"volvo\": \"ev_present\",\n",
    "    \"genesis\": \"ev_present\", \"leapmotor\": \"ev_active\", \"seres\": \"ev_active\", \"voyah\": \"ev_active\",\n",
    "    \"great\": \"ev_present\", \"gwm\": \"ev_present\", \"geely\": \"ev_present\", \"changan\": \"ev_present\",\n",
    "    \"honda\": \"low_ev\", \"toyota\": \"low_ev\", \"ford\": \"ev_present\", \"chevrolet\": \"ev_present\",\n",
    "    \"renault\": \"ev_present\", \"smart\": \"ev_present\", \"porsche\": \"ev_present\", \"mini\": \"ev_present\",\n",
    "    # fallback for others will be handled in the function\n",
    "}\n",
    "\n",
    "brand_orientation: Dict[str, str] = {\n",
    "    \"ferrari\": \"sport\", \"lamborghini\": \"sport\", \"mclaren\": \"sport\", \"lotus\": \"sport\",\n",
    "    \"porsche\": \"performance\", \"alpina\": \"performance\", \"dodge\": \"performance\", \"abarth\": \"performance\",\n",
    "    \"jeep\": \"utility_commercial\", \"foton\": \"utility_commercial\", \"gmc\": \"utility_commercial\", \"iveco\": \"utility_commercial\",\n",
    "    \"ford\": \"family\", \"toyota\": \"family\", \"honda\": \"family\", \"nissan\": \"family\", \"renault\": \"family\",\n",
    "    \"mini\": \"compact_city\", \"smart\": \"compact_city\", \"aixam\": \"compact_city\", \"microcar\": \"compact_city\",\n",
    "    \"rolls-royce\": \"luxury_grand_tourer\", \"bentley\": \"luxury_grand_tourer\", \"maybach\": \"luxury_grand_tourer\",\n",
    "    # unspecified brands will get default \"general\"\n",
    "}\n",
    "\n",
    "brand_status: Dict[str, str] = {\n",
    "    \"saab\": \"defunct\", \"pontiac\": \"defunct\", \"oldsmobile\": \"defunct\", \"plymouth\": \"defunct\",\n",
    "    \"daewoo\": \"defunct\", \"dkw\": \"defunct\", \"trabant\": \"defunct\", \"wartburg\": \"defunct\",\n",
    "    \"moskvich\": \"defunct\", \"rover\": \"defunct\", \"talbot\": \"defunct\", \"simca\": \"defunct\",\n",
    "    \"eagle\": \"defunct\", \"austin\": \"defunct\", \"tempo\": \"defunct\", \"volga\": \"defunct\",\n",
    "    \"други\": \"unknown\", \"asia\": \"unknown\"\n",
    "}\n",
    "def map_with_default(series: pd.Series, mapping: Dict[str, str], default: str,is_categorical=True) -> pd.Series:\n",
    "    mapped_series = series.map(lambda x: mapping.get(x, default))\n",
    "    if is_categorical:\n",
    "        return mapped_series.astype(\"category\")\n",
    "    else:\n",
    "        return mapped_series\n",
    "# --- Function to apply mappings ---\n",
    "def apply_brand_mappings(\n",
    "    df: pd.DataFrame,\n",
    "    brand_col: str = \"Brand\",\n",
    "    price_tier_map: Optional[Dict[str, str]] = None,\n",
    "    region_map: Optional[Dict[str, str]] = None,\n",
    "    ev_map: Optional[Dict[str, str]] = None,\n",
    "    orientation_map: Optional[Dict[str, str]] = None,\n",
    "    status_map: Optional[Dict[str, str]] = None,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # choose maps, prefer overrides\n",
    "    price_map = price_tier_map or brand_price_tier\n",
    "    region_map = region_map or brand_region\n",
    "    ev_map = ev_map or brand_ev_focus\n",
    "    orient_map = orientation_map or brand_orientation\n",
    "    status_map = status_map or brand_status\n",
    "\n",
    "\n",
    "    brand_series = df[brand_col]\n",
    "\n",
    "    df[\"brand_price_tier\"] = map_with_default(brand_series, price_map, \"unknown\")\n",
    "    df[\"brand_region\"] = map_with_default(brand_series, region_map, \"unknown\")\n",
    "    df[\"brand_ev_focus\"] = map_with_default(brand_series, ev_map, \"unknown\")\n",
    "    df[\"brand_orientation\"] = map_with_default(brand_series, orient_map, \"general\")\n",
    "    df[\"brand_status\"] = map_with_default(brand_series, status_map, \"active\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def handle_description(df):\n",
    "    with open('keywords.json',encoding='utf-8') as f:\n",
    "        keywords=json.load(f)\n",
    "    bad_tokens = keywords['bad_words'] \n",
    "    good_tokens = keywords['good_words']\n",
    "    bad_re = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, bad_tokens)) + r\")\\b\", flags=re.I)\n",
    "    good_re = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, good_tokens)) + r\")\\b\", flags=re.I)\n",
    "\n",
    "    def count_matches(text, pattern):\n",
    "        if not isinstance(text, str):\n",
    "            return 0\n",
    "        return len(pattern.findall(text))\n",
    "\n",
    "    # предполагаме df.description съдържа описанията\n",
    "\n",
    "    df[\"bad_count\"] = df[\"CarDescription\"].apply(lambda t: count_matches(t, bad_re))\n",
    "    df[\"good_count\"] = df[\"CarDescription\"].apply(lambda t: count_matches(t, good_re))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1db6434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_from_numeric_columns(df,columns,drop=False,std_multiplier=3):\n",
    "    for column in columns:\n",
    "        if drop:\n",
    "            df = df.drop(df[df[column] >= df[column].mean() + std_multiplier * df[column].std()].index)\n",
    "            df = df.drop(df[df[column] <= df[column].mean() - std_multiplier * df[column].std()].index)\n",
    "        else:\n",
    "            df[column].loc[df[column] >= df[column].mean() + std_multiplier * df[column].std()] = np.nan\n",
    "            df[column].loc[df[column] <= df[column].mean() - std_multiplier * df[column].std()] = np.nan\n",
    "    return df\n",
    "\n",
    "def plot_importance(model,num_features=50):\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature':model.feature_names_in_,\n",
    "        'importance':model.feature_importances_\n",
    "    }).sort_values('importance',ascending=False)\n",
    "    plt.figure(figsize=(16, 16), dpi=80)\n",
    "    plt.title('Feature Importance Most Important')\n",
    "    plt.barh(feature_importance_df['feature'][:num_features],feature_importance_df['importance'][:num_features])\n",
    "    plt.figure(figsize=(16, 16), dpi=80)\n",
    "    plt.title('Feature Importance Least Important')\n",
    "    plt.xlim(right=0.01)\n",
    "    plt.barh(feature_importance_df['feature'][-num_features:],feature_importance_df['importance'][-num_features:])\n",
    "\n",
    "    feature_importance_df['InitialColumn'] = feature_importance_df['feature'].str.split('_').str[0]\n",
    "    aggregated = feature_importance_df.groupby('InitialColumn')['importance'].sum().sort_values(ascending=False)\n",
    "    plt.figure(figsize=(16, 16), dpi=80)\n",
    "    plt.title('Aggregated Feature Importance by Initial Column')\n",
    "    plt.barh(aggregated.index[:num_features],aggregated.values[:num_features])\n",
    "    return feature_importance_df,aggregated\n",
    "\n",
    "def sumarize_small_values_in_column(df,column,threshold=0.01):\n",
    "    value_counts = df[column].value_counts(normalize=True)\n",
    "    small_values = value_counts[value_counts < threshold].index\n",
    "    df[column] = df[column].replace(small_values, 'Other')\n",
    "    return df, small_values\n",
    "\n",
    "def impute_missing_numerical_values(df, numerical_columns):\n",
    "    for col in numerical_columns:\n",
    "        median = df[col].median()\n",
    "        df[col] = df[col].fillna(median)\n",
    "    return df\n",
    "\n",
    "def get_split_data(df, target_column,seed=42):\n",
    "    y=df[target_column]\n",
    "    X=df.drop(columns=[target_column])\n",
    "    return train_test_split(X, y, test_size=0.3, random_state=seed,shuffle=True)\n",
    "\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test,train_model=True,is_log_transform=False,display_graphs=True):\n",
    "    if train_model:\n",
    "        model.fit(X_train, y_train)\n",
    "    results_train = model.predict(X_train)\n",
    "    results_test = model.predict(X_test)\n",
    "    if is_log_transform:\n",
    "        results_train = np.exp(results_train)\n",
    "        results_test = np.exp(results_test)\n",
    "        y_train = np.exp(y_train)\n",
    "        y_test = np.exp(y_test)\n",
    "    train_r2 = r2_score(y_train,results_train)\n",
    "    test_r2 = r2_score(y_test,results_test)\n",
    "    train_rmse = root_mean_squared_error(y_train,results_train)\n",
    "    test_rmse = root_mean_squared_error(y_test,results_test)\n",
    "    train_mae = mean_absolute_error(y_train,results_train)\n",
    "    test_mae = mean_absolute_error(y_test,results_test)\n",
    "    train_score_adj_r2 = 1 - (1 - train_r2) * (len(y_train) - 1) / (len(y_train) - X_train.shape[1] - 1)\n",
    "    test_score_adj_r2 = 1 - (1 - test_r2) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)\n",
    "\n",
    "    print(f\"Train R2: {round(train_r2, 4)}\")\n",
    "    print(f\"Train Adjusted R2: {round(train_score_adj_r2, 4)}\")\n",
    "    print(f\"Train RMSE: {round(train_rmse, 4)}\")\n",
    "    print(f\"Train MAE: {round(train_mae,4)}\")\n",
    "    print(f\"Test R2: {round(test_r2, 4)}\")\n",
    "    print(f\"Test Adjusted R2: {round(test_score_adj_r2, 4)}\")\n",
    "    print(f\"Test RMSE: {round(test_rmse, 4)}\")\n",
    "    print(f\"Test MAE: {round(test_mae,4)}\")\n",
    "    if display_graphs:\n",
    "        sns.regplot(x=y_test, y=results_test)\n",
    "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        residuals = y_test-results_test\n",
    "        sns.histplot(residuals,bins=100)\n",
    "    return train_r2,train_score_adj_r2,train_rmse,train_mae,test_r2,test_score_adj_r2,test_rmse,test_mae\n",
    "\n",
    "def add_features(df):\n",
    "    CURRENT_YEAR = 2025\n",
    "\n",
    "    df['KmPerYear'] = df['KmDriven'] / (CURRENT_YEAR - df['Year']).replace(0, 1)\n",
    "    df['HighKmPerYearFlag'] = (df['KmPerYear'] > df['KmPerYear'].quantile(0.95))\n",
    "    df['LowKmPerYearFlag'] = (df['KmPerYear'] < df['KmPerYear'].quantile(0.05))\n",
    "    df['Hoursepower_per_km'] = (df['Horsepower'] / df['KmDriven'].replace(0, 1))\n",
    "    df['HP_per_cc'] = df['Horsepower'] / df['CubicCapacity'].replace(0, np.nan)\n",
    "    df['Hoursepower_per_age'] = df['Horsepower'] /  (CURRENT_YEAR - df['Year']).replace(0, 1)\n",
    "\n",
    "    df['bm_median_km'] = df.groupby(['Brand','Model'])['KmDriven'].transform('median')\n",
    "\n",
    "    df['Km_vs_bm_median'] = df['KmDriven'] / df['bm_median_km'].replace(0, 1)\n",
    "\n",
    "\n",
    "\n",
    "def read_data(file_path,drop_URL=True,drop_model_column=True,impute_missing=True,dummies_drop_first=True,handle_brand=False,model_plus_brand=False,add_new_features=False,drop_condition_col=True,keywords=False,drop_brand=False):\n",
    "    df = pd.read_csv(file_path)\n",
    "    # New feature: isElectric\n",
    "    df['isElectric'] = ~df['WLTP_Range_km'].isna() | ~df['Battery_Capacity_kWh'].isna()\n",
    "    # Impute missing values for electric car features with 0\n",
    "    df['WLTP_Range_km'] = df['WLTP_Range_km'].fillna(0)\n",
    "    df['Battery_Capacity_kWh'] = df['Battery_Capacity_kWh'].fillna(0)\n",
    "    # Not used columns\n",
    "\n",
    "    if not drop_model_column:\n",
    "        model_processed=df['Model'].str.split(' ').str[1].str.lower().str.strip()\n",
    "    df['Model'] = df['Model'].str.split(' ').str[1].str.lower().str.strip()\n",
    "\n",
    "\n",
    "    if handle_brand:\n",
    "        brand_processed = df['Brand'].str.lower().str.strip()\n",
    "        df['Brand'] = brand_processed\n",
    "        df=apply_brand_mappings(df)\n",
    "        df,small_values_brand = sumarize_small_values_in_column(df,'Brand')\n",
    "\n",
    "    if model_plus_brand:\n",
    "        df['Brand_Model']=brand_processed+'_' + model_processed\n",
    "        with open('model_price_tier.json') as f:\n",
    "            model_price_mapping = json.load(f)\n",
    "        df[\"model_price_tier\"] = map_with_default(df[\"Brand_Model\"], model_price_mapping, \"unknown\")\n",
    "        with open('model_class_mapping.json') as f:\n",
    "            model_class_mapping = json.load(f)\n",
    "        df[\"model_class\"] = map_with_default(df[\"Brand_Model\"], model_class_mapping, \"unknown\")\n",
    "        with open('model_price_score.json') as f:\n",
    "            model_price_score_mapping = json.load(f)\n",
    "        df[\"model_price_score\"] = map_with_default(df[\"Brand_Model\"], model_price_score_mapping, None).astype(float)\n",
    "        df = df.drop(columns=['Brand_Model'])\n",
    "    if add_new_features:\n",
    "        add_features(df)\n",
    "    df = df.drop(columns=['Month']) # Not useful for prediction\n",
    "    if not drop_URL:\n",
    "        url =df['URL']\n",
    "    df = df.drop(columns=['URL']) # Not useful for prediction\n",
    "    df = df.drop(columns=['Area']) # Very correlated with City and City has less missing values\n",
    "    df = df.drop(columns=['Model']) # Too many unique values\n",
    "    df = df.drop(columns=['Status']) # Artificial column created by mobile.bg\n",
    "    if keywords:\n",
    "        handle_description(df)\n",
    "    if 'CarDescription' in df.columns:\n",
    "        df =df.drop(columns=['CarDescription'])\n",
    "    if drop_condition_col:\n",
    "        df = df.drop(columns=['Condition']) # 97% empty\n",
    "    if impute_missing:\n",
    "        df = impute_missing_numerical_values(df, ['Year', 'Horsepower', 'CubicCapacity', 'KmDriven'])\n",
    "    if drop_brand:\n",
    "        df = df.drop(columns=['Brand'])\n",
    "    print(\"Before onehot encoding\",df.shape)\n",
    "    df = pd.get_dummies(df,drop_first=dummies_drop_first)\n",
    "    print(\"After onehot encoding\",df.shape)\n",
    "    if not drop_model_column:\n",
    "        df['Model'] = model_processed\n",
    "\n",
    "    if handle_brand:\n",
    "        df['Brand'] = brand_processed\n",
    "    if not drop_URL:\n",
    "        df['URL']=url\n",
    "    return df\n",
    "\n",
    "def handle_model_column(X_train, X_test,column_name='Model'):\n",
    "    counts = X_train[column_name].value_counts()\n",
    "    counts = counts[counts >= 10]\n",
    "    possible_values = set(counts.index.tolist())\n",
    "    X_train.loc[~X_train[column_name].isin(possible_values), column_name] = 'Other'\n",
    "    X_test.loc[~X_test[column_name].isin(possible_values), column_name] = 'Other'\n",
    "\n",
    "    values = X_train.groupby(column_name)['CarPrice'].agg(['mean','std']).sort_values(by='mean', ascending=False)\n",
    "\n",
    "    X_train['MeanPriceByModel'] = X_train[column_name].map(values['mean'])\n",
    "    X_train['StdPriceByModel'] = X_train[column_name].map(values['std'])\n",
    "    X_train['VariancePriceByModel'] = X_train[column_name].map(values['std']**2)\n",
    "    X_test['MeanPriceByModel'] = X_test[column_name].map(values['mean'])\n",
    "    X_test['StdPriceByModel'] = X_test[column_name].map(values['std'])\n",
    "    X_test['VariancePriceByModel'] = X_test[column_name].map(values['std']**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76b9f1c",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b42e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdc12b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51432cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8a086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODELS:\n",
    "    df = read_data('processed_data/preprocessed_car_data_full_brand_with_outliers.csv',drop_model_column=False,impute_missing=False,dummies_drop_first=False,handle_brand=True,model_plus_brand=True,add_new_features=True,drop_condition_col=True)\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = get_split_data(df, 'CarPrice',seed=42)\n",
    "\n",
    "    #y_train = np.log(y_train)\n",
    "    #y_test = np.log(y_test)\n",
    "    X_train['CarPrice'] = y_train\n",
    "    handle_model_column(X_train, X_test)\n",
    "    #handle_model_column(X_train, X_test,column_name='Brand')\n",
    "\n",
    "    X_train = X_train.drop(columns=['CarPrice'])\n",
    "    X_train = X_train.drop(columns=['Model'])\n",
    "    X_test = X_test.drop(columns=['Model'])\n",
    "    X_train = X_train.drop(columns=['Brand'])\n",
    "    X_test = X_test.drop(columns=['Brand'])\n",
    "\n",
    "    imputer = SimpleImputer()\n",
    "    imputer.fit(X_train)\n",
    "    X_train=imputer.transform(X_train)\n",
    "    X_test=imputer.transform(X_test)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train=scaler.transform(X_train)\n",
    "    X_test=scaler.transform(X_test)\n",
    "    print(\"Final training shape:\",X_train.shape)\n",
    "    #final_model.fit(X_train, y_train,verbose=True,eval_set=[(X_test, y_test)])\n",
    "    train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_train).to(dtype=torch.float32,device=device),torch.from_numpy(y_train.to_numpy()).to(dtype=torch.float32,device=device))\n",
    "    test_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_test).to(dtype=torch.float32,device=device),torch.from_numpy(y_test.to_numpy()).to(dtype=torch.float32,device=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32ee8e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_dataloader,model,loss_fn,y_mean):\n",
    "    cum_loss=0\n",
    "    r_sq_up=0\n",
    "    r_sq_down=0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X,y in test_dataloader:\n",
    "            res = model(X).view(-1)\n",
    "            loss = loss_fn(res,y)\n",
    "            cum_loss+=loss\n",
    "\n",
    "            r_sq_up+=((y-res)**2).sum()\n",
    "            r_sq_down+=((y-y_mean)**2).sum()\n",
    "\n",
    "    return (cum_loss/len(test_dataloader)).item(),(1-(r_sq_up/r_sq_down)).item()\n",
    "\n",
    "\n",
    "def train(dataloader,model,loss_fn,optimizer,epochs,print_step=100,test_dataloader=None):\n",
    "    all_losses=[]\n",
    "    all_test_losses=[]\n",
    "    test_r_sq=[]\n",
    "    train_r_sq=[]\n",
    "    if test_dataloader is not None:\n",
    "        n_samples = 0\n",
    "        sum_y = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X, y in test_dataloader:\n",
    "                y = y.to(device)\n",
    "                y_flat = y.view(-1)\n",
    "                sum_y += y_flat.sum().item()\n",
    "                n_samples += y_flat.shape[0]\n",
    "\n",
    "            if n_samples == 0:\n",
    "                return float('nan'), float('nan')\n",
    "            y_mean = sum_y / n_samples\n",
    "\n",
    "\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        current_losses=[]\n",
    "        iteration=0\n",
    "        r_sq_up=0\n",
    "        r_sq_down=0\n",
    "        for X,y in dataloader:\n",
    "            model.eval()\n",
    "            res = model(X).view(-1)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = loss_fn(res,y)\n",
    "            current_losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            r_sq_up+=((y-res)**2).sum()\n",
    "            r_sq_down+=((y-y_mean)**2).sum()\n",
    "            iteration+=1\n",
    "            if iteration%print_step==0:\n",
    "                print(f\"Epoch:{i} Iteration:{iteration} Loss:{loss.item()}\")\n",
    "\n",
    "        if test_dataloader is not None:\n",
    "            test_loss,r_sq = test(test_dataloader,model,loss_fn,y_mean)\n",
    "            all_test_losses.append(test_loss)\n",
    "            test_r_sq.append(r_sq)\n",
    "            r_sq_train=(1-(r_sq_up/r_sq_down)).item()\n",
    "            train_r_sq.append(r_sq_train)\n",
    "            print(f\"Epoch:{i} Test Loss:{test_loss}, Train R^2:{r_sq_train}, Test R^2:{r_sq}\")\n",
    "\n",
    "        all_losses.append(current_losses)\n",
    "\n",
    "    return all_losses,all_test_losses,test_r_sq,train_r_sq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58c7b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODELS:\n",
    "    BATCH_SIZE=32\n",
    "    learning_rate=0.001\n",
    "    hidden_size=64\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "    network=torch.nn.Sequential(\n",
    "        torch.nn.Linear(X_train.shape[1],hidden_size*2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(0.5),\n",
    "        torch.nn.BatchNorm1d(hidden_size*2),\n",
    "        torch.nn.Linear(hidden_size*2,hidden_size*4),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(0.5),\n",
    "        torch.nn.BatchNorm1d(hidden_size*4),\n",
    "        torch.nn.Linear(hidden_size*4,hidden_size*8),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.BatchNorm1d(hidden_size*8),\n",
    "        torch.nn.Linear(hidden_size*8,1),\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.Adam(network.parameters(),lr=learning_rate)\n",
    "    loss = torch.nn.functional.mse_loss\n",
    "    train_res,test_res,test_r_sq,train_r_sq = train(train_dataloader,network,loss,optimizer,epochs=50,test_dataloader=test_dataloader)\n",
    "\n",
    "    result = torch.tensor(train_res).flatten()\n",
    "    elements = result.shape[0]\n",
    "    window_size = 50\n",
    "    to_plot=result[-window_size*(elements//window_size):].view(-1,window_size).mean(-1)\n",
    "    plt.plot(to_plot)\n",
    "    plt.show()\n",
    "    plt.plot(test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59cb959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "504f2d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before onehot encoding (112477, 121)\n",
      "After onehot encoding (112477, 216)\n",
      "Final training shape: (78733, 218)\n"
     ]
    }
   ],
   "source": [
    "brand_column='Brand'\n",
    "df = read_data('processed_data/preprocessed_car_data_full_brand_with_outliers.csv',drop_model_column=False,impute_missing=False,dummies_drop_first=True,handle_brand=True,model_plus_brand=True,add_new_features=False,drop_condition_col=True,drop_brand=True)\n",
    "brand = pd.read_csv('processed_data/preprocessed_car_data_full_brand_with_outliers.csv')[brand_column]\n",
    "label_encoder = LabelEncoder()\n",
    "brand=label_encoder.fit_transform(brand)\n",
    "df[brand_column]=brand\n",
    "X_train, X_test, y_train, y_test = get_split_data(df, 'CarPrice',seed=42)\n",
    "\n",
    "train_brand = X_train[brand_column]\n",
    "\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)\n",
    "X_train['CarPrice'] = y_train\n",
    "handle_model_column(X_train, X_test)\n",
    "#handle_model_column(X_train, X_test,column_name='Brand')\n",
    "\n",
    "X_train = X_train.drop(columns=['CarPrice'])\n",
    "X_train = X_train.drop(columns=['Model'])\n",
    "X_test = X_test.drop(columns=['Model'])\n",
    "X_train = X_train.drop(columns=['Brand'])\n",
    "X_test = X_test.drop(columns=['Brand'])\n",
    "\n",
    "imputer = SimpleImputer()\n",
    "imputer.fit(X_train)\n",
    "X_train=imputer.transform(X_train)\n",
    "X_test=imputer.transform(X_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train=scaler.transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "print(\"Final training shape:\",X_train.shape)\n",
    "#final_model.fit(X_train, y_train,verbose=True,eval_set=[(X_test, y_test)])\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_train).to(dtype=torch.float32,device=device),torch.from_numpy(y_train.to_numpy()).to(dtype=torch.float32,device=device),torch.from_numpy(train_brand.to_numpy()).to(dtype=torch.int32,device=device))\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_test).to(dtype=torch.float32,device=device),torch.from_numpy(y_test.to_numpy()).to(dtype=torch.float32,device=device),torch.from_numpy(test_brand.to_numpy()).to(dtype=torch.int32,device=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12a96198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_dataloader,model,loss_fn,y_mean):\n",
    "    cum_loss=0\n",
    "    r_sq_up=0\n",
    "    r_sq_down=0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X,y,brand in test_dataloader:\n",
    "            res = model(X,brand).view(-1)\n",
    "            loss = loss_fn(res,y)\n",
    "            cum_loss+=loss\n",
    "\n",
    "            r_sq_up+=((y-res)**2).sum()\n",
    "            r_sq_down+=((y-y_mean)**2).sum()\n",
    "\n",
    "    return (cum_loss/len(test_dataloader)).item(),(1-(r_sq_up/r_sq_down)).item()\n",
    "\n",
    "\n",
    "def train(dataloader,model,loss_fn,optimizer,epochs,print_step=100,test_dataloader=None):\n",
    "    all_losses=[]\n",
    "    all_test_losses=[]\n",
    "    test_r_sq=[]\n",
    "    train_r_sq=[]\n",
    "    if test_dataloader is not None:\n",
    "        n_samples = 0\n",
    "        sum_y = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X, y,brand in test_dataloader:\n",
    "                y = y.to(device)\n",
    "                y_flat = y.view(-1)\n",
    "                sum_y += y_flat.sum().item()\n",
    "                n_samples += y_flat.shape[0]\n",
    "\n",
    "            if n_samples == 0:\n",
    "                return float('nan'), float('nan')\n",
    "            y_mean = sum_y / n_samples\n",
    "\n",
    "\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        current_losses=[]\n",
    "        iteration=0\n",
    "        r_sq_up=0\n",
    "        r_sq_down=0\n",
    "        for X,y,brand in dataloader:\n",
    "            model.eval()\n",
    "            res = model(X,brand).view(-1)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = loss_fn(res,y)\n",
    "            current_losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            r_sq_up+=((y-res)**2).sum()\n",
    "            r_sq_down+=((y-y_mean)**2).sum()\n",
    "            iteration+=1\n",
    "            if iteration%print_step==0:\n",
    "                print(f\"Epoch:{i} Iteration:{iteration} Loss:{loss.item()}\")\n",
    "\n",
    "        if test_dataloader is not None:\n",
    "            test_loss,r_sq = test(test_dataloader,model,loss_fn,y_mean)\n",
    "            all_test_losses.append(test_loss)\n",
    "            test_r_sq.append(r_sq)\n",
    "            r_sq_train=(1-(r_sq_up/r_sq_down)).item()\n",
    "            train_r_sq.append(r_sq_train)\n",
    "            print(f\"Epoch:{i} Test Loss:{test_loss}, Train R^2:{r_sq_train}, Test R^2:{r_sq}\")\n",
    "\n",
    "        all_losses.append(current_losses)\n",
    "\n",
    "    return all_losses,all_test_losses,test_r_sq,train_r_sq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5942961",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,input_shape,brand_dim,embed_dim,hidden_size):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(brand_dim,embed_dim)\n",
    "        self.network=torch.nn.Sequential(\n",
    "            torch.nn.Linear(embed_dim+input_shape,hidden_size*2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.BatchNorm1d(hidden_size*2),\n",
    "            torch.nn.Linear(hidden_size*2,hidden_size*4),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.BatchNorm1d(hidden_size*4),\n",
    "            torch.nn.Linear(hidden_size*4,hidden_size*8),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(hidden_size*8),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(hidden_size*8,1),\n",
    "        )\n",
    "\n",
    "    def forward(self,X,brand):\n",
    "        brand_embed=self.embed(brand)\n",
    "        return self.network(torch.concat((X,brand_embed),dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cb7a59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Iteration:100 Loss:0.47796517610549927\n",
      "Epoch:0 Iteration:200 Loss:0.38129469752311707\n",
      "Epoch:0 Iteration:300 Loss:0.1558576226234436\n",
      "Epoch:0 Iteration:400 Loss:0.18258671462535858\n",
      "Epoch:0 Iteration:500 Loss:0.13677111268043518\n",
      "Epoch:0 Iteration:600 Loss:0.2153133749961853\n",
      "Epoch:0 Iteration:700 Loss:0.23336416482925415\n",
      "Epoch:0 Iteration:800 Loss:0.2695659399032593\n",
      "Epoch:0 Iteration:900 Loss:0.219628244638443\n",
      "Epoch:0 Iteration:1000 Loss:0.16710378229618073\n",
      "Epoch:0 Iteration:1100 Loss:0.23915132880210876\n",
      "Epoch:0 Iteration:1200 Loss:0.10365127772092819\n",
      "Epoch:0 Iteration:1300 Loss:0.11297035962343216\n",
      "Epoch:0 Iteration:1400 Loss:0.223992720246315\n",
      "Epoch:0 Iteration:1500 Loss:0.10056393593549728\n",
      "Epoch:0 Iteration:1600 Loss:0.1247183308005333\n",
      "Epoch:0 Iteration:1700 Loss:0.1510932743549347\n",
      "Epoch:0 Iteration:1800 Loss:0.18114829063415527\n",
      "Epoch:0 Iteration:1900 Loss:0.11786503344774246\n",
      "Epoch:0 Iteration:2000 Loss:0.14040938019752502\n",
      "Epoch:0 Iteration:2100 Loss:0.18400511145591736\n",
      "Epoch:0 Iteration:2200 Loss:0.07399185001850128\n",
      "Epoch:0 Iteration:2300 Loss:0.08933737128973007\n",
      "Epoch:0 Iteration:2400 Loss:0.15094222128391266\n",
      "Epoch:0 Test Loss:0.1130601167678833, Train R^2:0.4314112663269043, Test R^2:0.8972444534301758\n",
      "Epoch:1 Iteration:100 Loss:0.08819997310638428\n",
      "Epoch:1 Iteration:200 Loss:0.05956782400608063\n",
      "Epoch:1 Iteration:300 Loss:0.07675178349018097\n",
      "Epoch:1 Iteration:400 Loss:0.06969425827264786\n",
      "Epoch:1 Iteration:500 Loss:0.08496058732271194\n",
      "Epoch:1 Iteration:600 Loss:0.10115016251802444\n",
      "Epoch:1 Iteration:700 Loss:0.07651058584451675\n",
      "Epoch:1 Iteration:800 Loss:0.056857623159885406\n",
      "Epoch:1 Iteration:900 Loss:0.13461758196353912\n",
      "Epoch:1 Iteration:1000 Loss:0.12900501489639282\n",
      "Epoch:1 Iteration:1100 Loss:0.06394566595554352\n",
      "Epoch:1 Iteration:1200 Loss:0.07233322411775589\n",
      "Epoch:1 Iteration:1300 Loss:0.06556317210197449\n",
      "Epoch:1 Iteration:1400 Loss:0.053589724004268646\n",
      "Epoch:1 Iteration:1500 Loss:0.22090129554271698\n",
      "Epoch:1 Iteration:1600 Loss:0.04923148825764656\n",
      "Epoch:1 Iteration:1700 Loss:0.04929332435131073\n",
      "Epoch:1 Iteration:1800 Loss:0.08284330368041992\n",
      "Epoch:1 Iteration:1900 Loss:0.06627035140991211\n",
      "Epoch:1 Iteration:2000 Loss:0.10383635759353638\n",
      "Epoch:1 Iteration:2100 Loss:0.21016106009483337\n",
      "Epoch:1 Iteration:2200 Loss:0.06049438193440437\n",
      "Epoch:1 Iteration:2300 Loss:0.05109168589115143\n",
      "Epoch:1 Iteration:2400 Loss:0.07858245074748993\n",
      "Epoch:1 Test Loss:0.08557762205600739, Train R^2:0.910870373249054, Test R^2:0.922227144241333\n",
      "Epoch:2 Iteration:100 Loss:0.057088837027549744\n",
      "Epoch:2 Iteration:200 Loss:0.11180776357650757\n",
      "Epoch:2 Iteration:300 Loss:0.06278616189956665\n",
      "Epoch:2 Iteration:400 Loss:0.0916840210556984\n",
      "Epoch:2 Iteration:500 Loss:0.045188888907432556\n",
      "Epoch:2 Iteration:600 Loss:0.19213257730007172\n",
      "Epoch:2 Iteration:700 Loss:0.06573699414730072\n",
      "Epoch:2 Iteration:800 Loss:0.15534992516040802\n",
      "Epoch:2 Iteration:900 Loss:0.09423737227916718\n",
      "Epoch:2 Iteration:1000 Loss:0.14503207802772522\n",
      "Epoch:2 Iteration:1100 Loss:0.09557277709245682\n",
      "Epoch:2 Iteration:1200 Loss:0.13638822734355927\n",
      "Epoch:2 Iteration:1300 Loss:0.10273437947034836\n",
      "Epoch:2 Iteration:1400 Loss:0.10064384341239929\n",
      "Epoch:2 Iteration:1500 Loss:0.04788084700703621\n",
      "Epoch:2 Iteration:1600 Loss:0.07722742855548859\n",
      "Epoch:2 Iteration:1700 Loss:0.04653804376721382\n",
      "Epoch:2 Iteration:1800 Loss:0.06134860962629318\n",
      "Epoch:2 Iteration:1900 Loss:0.0729728639125824\n",
      "Epoch:2 Iteration:2000 Loss:0.0839516744017601\n",
      "Epoch:2 Iteration:2100 Loss:0.1369527280330658\n",
      "Epoch:2 Iteration:2200 Loss:0.05338223651051521\n",
      "Epoch:2 Iteration:2300 Loss:0.09086748957633972\n",
      "Epoch:2 Iteration:2400 Loss:0.08096128702163696\n",
      "Epoch:2 Test Loss:0.09183313697576523, Train R^2:0.9230665564537048, Test R^2:0.9165250658988953\n",
      "Epoch:3 Iteration:100 Loss:0.0927424430847168\n",
      "Epoch:3 Iteration:200 Loss:0.04275943338871002\n",
      "Epoch:3 Iteration:300 Loss:0.06405173242092133\n",
      "Epoch:3 Iteration:400 Loss:0.11165773868560791\n",
      "Epoch:3 Iteration:500 Loss:0.07462956011295319\n",
      "Epoch:3 Iteration:600 Loss:0.083573117852211\n",
      "Epoch:3 Iteration:700 Loss:0.06023908779025078\n",
      "Epoch:3 Iteration:800 Loss:0.09966906905174255\n",
      "Epoch:3 Iteration:900 Loss:0.09664824604988098\n",
      "Epoch:3 Iteration:1000 Loss:0.09921744465827942\n",
      "Epoch:3 Iteration:1100 Loss:0.04214112088084221\n",
      "Epoch:3 Iteration:1200 Loss:0.04949232190847397\n",
      "Epoch:3 Iteration:1300 Loss:0.10714223980903625\n",
      "Epoch:3 Iteration:1400 Loss:0.059435389935970306\n",
      "Epoch:3 Iteration:1500 Loss:0.09105090796947479\n",
      "Epoch:3 Iteration:1600 Loss:0.052854642271995544\n",
      "Epoch:3 Iteration:1700 Loss:0.16740453243255615\n",
      "Epoch:3 Iteration:1800 Loss:0.07448767125606537\n",
      "Epoch:3 Iteration:1900 Loss:0.12512877583503723\n",
      "Epoch:3 Iteration:2000 Loss:0.07261431962251663\n",
      "Epoch:3 Iteration:2100 Loss:0.05274476110935211\n",
      "Epoch:3 Iteration:2200 Loss:0.08642739802598953\n",
      "Epoch:3 Iteration:2300 Loss:0.05448751151561737\n",
      "Epoch:3 Iteration:2400 Loss:0.13070140779018402\n",
      "Epoch:3 Test Loss:0.0834258496761322, Train R^2:0.9285359978675842, Test R^2:0.9242002964019775\n",
      "Epoch:4 Iteration:100 Loss:0.126628115773201\n",
      "Epoch:4 Iteration:200 Loss:0.11308043450117111\n",
      "Epoch:4 Iteration:300 Loss:0.047100991010665894\n",
      "Epoch:4 Iteration:400 Loss:0.06346435844898224\n",
      "Epoch:4 Iteration:500 Loss:0.0641123354434967\n",
      "Epoch:4 Iteration:600 Loss:0.049789272248744965\n",
      "Epoch:4 Iteration:700 Loss:0.08468000590801239\n",
      "Epoch:4 Iteration:800 Loss:0.06795132160186768\n",
      "Epoch:4 Iteration:900 Loss:0.04527714475989342\n",
      "Epoch:4 Iteration:1000 Loss:0.044679950922727585\n",
      "Epoch:4 Iteration:1100 Loss:0.054085493087768555\n",
      "Epoch:4 Iteration:1200 Loss:0.05901423096656799\n",
      "Epoch:4 Iteration:1300 Loss:0.0696779265999794\n",
      "Epoch:4 Iteration:1400 Loss:0.1197935938835144\n",
      "Epoch:4 Iteration:1500 Loss:0.13054385781288147\n",
      "Epoch:4 Iteration:1600 Loss:0.050159066915512085\n",
      "Epoch:4 Iteration:1700 Loss:0.1063356101512909\n",
      "Epoch:4 Iteration:1800 Loss:0.04206227883696556\n",
      "Epoch:4 Iteration:1900 Loss:0.07773727178573608\n",
      "Epoch:4 Iteration:2000 Loss:0.06921719014644623\n",
      "Epoch:4 Iteration:2100 Loss:0.0345601812005043\n",
      "Epoch:4 Iteration:2200 Loss:0.037255946546792984\n",
      "Epoch:4 Iteration:2300 Loss:0.05973781272768974\n",
      "Epoch:4 Iteration:2400 Loss:0.059578508138656616\n",
      "Epoch:4 Test Loss:0.07445836812257767, Train R^2:0.9332684874534607, Test R^2:0.9323173761367798\n",
      "Epoch:5 Iteration:100 Loss:0.08710749447345734\n",
      "Epoch:5 Iteration:200 Loss:0.04974672943353653\n",
      "Epoch:5 Iteration:300 Loss:0.18337978422641754\n",
      "Epoch:5 Iteration:400 Loss:0.06993311643600464\n",
      "Epoch:5 Iteration:500 Loss:0.04801075905561447\n",
      "Epoch:5 Iteration:600 Loss:0.07974233478307724\n",
      "Epoch:5 Iteration:700 Loss:0.12322738766670227\n",
      "Epoch:5 Iteration:800 Loss:0.052979037165641785\n",
      "Epoch:5 Iteration:900 Loss:0.05193321779370308\n",
      "Epoch:5 Iteration:1000 Loss:0.06846641004085541\n",
      "Epoch:5 Iteration:1100 Loss:0.043139759451150894\n",
      "Epoch:5 Iteration:1200 Loss:0.18201205134391785\n",
      "Epoch:5 Iteration:1300 Loss:0.0811867043375969\n",
      "Epoch:5 Iteration:1400 Loss:0.09414467215538025\n",
      "Epoch:5 Iteration:1500 Loss:0.07312459498643875\n",
      "Epoch:5 Iteration:1600 Loss:0.08229733258485794\n",
      "Epoch:5 Iteration:1700 Loss:0.04479362070560455\n",
      "Epoch:5 Iteration:1800 Loss:0.03404275327920914\n",
      "Epoch:5 Iteration:1900 Loss:0.05053972452878952\n",
      "Epoch:5 Iteration:2000 Loss:0.06429385393857956\n",
      "Epoch:5 Iteration:2100 Loss:0.10447081923484802\n",
      "Epoch:5 Iteration:2200 Loss:0.04669702798128128\n",
      "Epoch:5 Iteration:2300 Loss:0.054323937743902206\n",
      "Epoch:5 Iteration:2400 Loss:0.06013499200344086\n",
      "Epoch:5 Test Loss:0.08091861754655838, Train R^2:0.9380667209625244, Test R^2:0.9264542460441589\n",
      "Epoch:6 Iteration:100 Loss:0.060065411031246185\n",
      "Epoch:6 Iteration:200 Loss:0.12588514387607574\n",
      "Epoch:6 Iteration:300 Loss:0.05437077581882477\n",
      "Epoch:6 Iteration:400 Loss:0.03912379965186119\n",
      "Epoch:6 Iteration:500 Loss:0.03213494271039963\n",
      "Epoch:6 Iteration:600 Loss:0.12727370858192444\n",
      "Epoch:6 Iteration:700 Loss:0.0769754946231842\n",
      "Epoch:6 Iteration:800 Loss:0.040439724922180176\n",
      "Epoch:6 Iteration:900 Loss:0.04387694224715233\n",
      "Epoch:6 Iteration:1000 Loss:0.05658332258462906\n",
      "Epoch:6 Iteration:1100 Loss:0.03919076547026634\n",
      "Epoch:6 Iteration:1200 Loss:0.08767111599445343\n",
      "Epoch:6 Iteration:1300 Loss:0.03879239037632942\n",
      "Epoch:6 Iteration:1400 Loss:0.03704584389925003\n",
      "Epoch:6 Iteration:1500 Loss:0.076131671667099\n",
      "Epoch:6 Iteration:1600 Loss:0.08267366886138916\n",
      "Epoch:6 Iteration:1700 Loss:0.044372692704200745\n",
      "Epoch:6 Iteration:1800 Loss:0.05141206830739975\n",
      "Epoch:6 Iteration:1900 Loss:0.037353515625\n",
      "Epoch:6 Iteration:2000 Loss:0.06426644325256348\n",
      "Epoch:6 Iteration:2100 Loss:0.04326459392905235\n",
      "Epoch:6 Iteration:2200 Loss:0.03404342383146286\n",
      "Epoch:6 Iteration:2300 Loss:0.03886920213699341\n",
      "Epoch:6 Iteration:2400 Loss:0.03901326283812523\n",
      "Epoch:6 Test Loss:0.06969070434570312, Train R^2:0.9412237405776978, Test R^2:0.9366843104362488\n",
      "Epoch:7 Iteration:100 Loss:0.05600220710039139\n",
      "Epoch:7 Iteration:200 Loss:0.0726691484451294\n",
      "Epoch:7 Iteration:300 Loss:0.024711012840270996\n",
      "Epoch:7 Iteration:400 Loss:0.07343253493309021\n",
      "Epoch:7 Iteration:500 Loss:0.06054280698299408\n",
      "Epoch:7 Iteration:600 Loss:0.0629836916923523\n",
      "Epoch:7 Iteration:700 Loss:0.040532149374485016\n",
      "Epoch:7 Iteration:800 Loss:0.06513582915067673\n",
      "Epoch:7 Iteration:900 Loss:0.03263548016548157\n",
      "Epoch:7 Iteration:1000 Loss:0.04850081726908684\n",
      "Epoch:7 Iteration:1100 Loss:0.061969857662916183\n",
      "Epoch:7 Iteration:1200 Loss:0.08655082434415817\n",
      "Epoch:7 Iteration:1300 Loss:0.035959407687187195\n",
      "Epoch:7 Iteration:1400 Loss:0.06765756756067276\n",
      "Epoch:7 Iteration:1500 Loss:0.057269372045993805\n",
      "Epoch:7 Iteration:1600 Loss:0.04571050778031349\n",
      "Epoch:7 Iteration:1700 Loss:0.03425968438386917\n",
      "Epoch:7 Iteration:1800 Loss:0.07045203447341919\n",
      "Epoch:7 Iteration:1900 Loss:0.04816393926739693\n",
      "Epoch:7 Iteration:2000 Loss:0.08479651808738708\n",
      "Epoch:7 Iteration:2100 Loss:0.049007728695869446\n",
      "Epoch:7 Iteration:2200 Loss:0.08204187452793121\n",
      "Epoch:7 Iteration:2300 Loss:0.10613967478275299\n",
      "Epoch:7 Iteration:2400 Loss:0.07519526779651642\n",
      "Epoch:7 Test Loss:0.06851887702941895, Train R^2:0.9444940090179443, Test R^2:0.9377241134643555\n",
      "Epoch:8 Iteration:100 Loss:0.056167714297771454\n",
      "Epoch:8 Iteration:200 Loss:0.049681197851896286\n",
      "Epoch:8 Iteration:300 Loss:0.034982550889253616\n",
      "Epoch:8 Iteration:400 Loss:0.044819172471761703\n",
      "Epoch:8 Iteration:500 Loss:0.03074832446873188\n",
      "Epoch:8 Iteration:600 Loss:0.0616677924990654\n",
      "Epoch:8 Iteration:700 Loss:0.04653850942850113\n",
      "Epoch:8 Iteration:800 Loss:0.0485510490834713\n",
      "Epoch:8 Iteration:900 Loss:0.048758719116449356\n",
      "Epoch:8 Iteration:1000 Loss:0.17426659166812897\n",
      "Epoch:8 Iteration:1100 Loss:0.03791762888431549\n",
      "Epoch:8 Iteration:1200 Loss:0.08350302278995514\n",
      "Epoch:8 Iteration:1300 Loss:0.14630338549613953\n",
      "Epoch:8 Iteration:1400 Loss:0.049583081156015396\n",
      "Epoch:8 Iteration:1500 Loss:0.035925380885601044\n",
      "Epoch:8 Iteration:1600 Loss:0.03475096821784973\n",
      "Epoch:8 Iteration:1700 Loss:0.049348942935466766\n",
      "Epoch:8 Iteration:1800 Loss:0.08535884320735931\n",
      "Epoch:8 Iteration:1900 Loss:0.040712255984544754\n",
      "Epoch:8 Iteration:2000 Loss:0.04207064211368561\n",
      "Epoch:8 Iteration:2100 Loss:0.031181400641798973\n",
      "Epoch:8 Iteration:2200 Loss:0.04641585797071457\n",
      "Epoch:8 Iteration:2300 Loss:0.04987116903066635\n",
      "Epoch:8 Iteration:2400 Loss:0.026240838691592216\n",
      "Epoch:8 Test Loss:0.07719890773296356, Train R^2:0.9465230703353882, Test R^2:0.9298441410064697\n",
      "Epoch:9 Iteration:100 Loss:0.03533618524670601\n",
      "Epoch:9 Iteration:200 Loss:0.02926303818821907\n",
      "Epoch:9 Iteration:300 Loss:0.0309174582362175\n",
      "Epoch:9 Iteration:400 Loss:0.043101098388433456\n",
      "Epoch:9 Iteration:500 Loss:0.04890069365501404\n",
      "Epoch:9 Iteration:600 Loss:0.034004975110292435\n",
      "Epoch:9 Iteration:700 Loss:0.07138818502426147\n",
      "Epoch:9 Iteration:800 Loss:0.08586680889129639\n",
      "Epoch:9 Iteration:900 Loss:0.03804329037666321\n",
      "Epoch:9 Iteration:1000 Loss:0.059802036732435226\n",
      "Epoch:9 Iteration:1100 Loss:0.054976921528577805\n",
      "Epoch:9 Iteration:1200 Loss:0.03838050365447998\n",
      "Epoch:9 Iteration:1300 Loss:0.0643896609544754\n",
      "Epoch:9 Iteration:1400 Loss:0.04989653825759888\n",
      "Epoch:9 Iteration:1500 Loss:0.1117560863494873\n",
      "Epoch:9 Iteration:1600 Loss:0.04521394520998001\n",
      "Epoch:9 Iteration:1700 Loss:0.036897603422403336\n",
      "Epoch:9 Iteration:1800 Loss:0.03179723769426346\n",
      "Epoch:9 Iteration:1900 Loss:0.03279487043619156\n",
      "Epoch:9 Iteration:2000 Loss:0.0562015138566494\n",
      "Epoch:9 Iteration:2100 Loss:0.034682177007198334\n",
      "Epoch:9 Iteration:2200 Loss:0.0372643880546093\n",
      "Epoch:9 Iteration:2300 Loss:0.0454806350171566\n",
      "Epoch:9 Iteration:2400 Loss:0.05017939954996109\n",
      "Epoch:9 Test Loss:0.06689080595970154, Train R^2:0.9489123821258545, Test R^2:0.9392054677009583\n",
      "Epoch:10 Iteration:100 Loss:0.08169963955879211\n",
      "Epoch:10 Iteration:200 Loss:0.04489707946777344\n",
      "Epoch:10 Iteration:300 Loss:0.02381535992026329\n",
      "Epoch:10 Iteration:400 Loss:0.028217701241374016\n",
      "Epoch:10 Iteration:500 Loss:0.035038869827985764\n",
      "Epoch:10 Iteration:600 Loss:0.04224428907036781\n",
      "Epoch:10 Iteration:700 Loss:0.1505831480026245\n",
      "Epoch:10 Iteration:800 Loss:0.05198227986693382\n",
      "Epoch:10 Iteration:900 Loss:0.04858619347214699\n",
      "Epoch:10 Iteration:1000 Loss:0.04868410527706146\n",
      "Epoch:10 Iteration:1100 Loss:0.051123764365911484\n",
      "Epoch:10 Iteration:1200 Loss:0.04980403184890747\n",
      "Epoch:10 Iteration:1300 Loss:0.1167997196316719\n",
      "Epoch:10 Iteration:1400 Loss:0.0643608421087265\n",
      "Epoch:10 Iteration:1500 Loss:0.04916554316878319\n",
      "Epoch:10 Iteration:1600 Loss:0.048099882900714874\n",
      "Epoch:10 Iteration:1700 Loss:0.03754112124443054\n",
      "Epoch:10 Iteration:1800 Loss:0.04853174462914467\n",
      "Epoch:10 Iteration:1900 Loss:0.12475103884935379\n",
      "Epoch:10 Iteration:2000 Loss:0.031574077904224396\n",
      "Epoch:10 Iteration:2100 Loss:0.05442798137664795\n",
      "Epoch:10 Iteration:2200 Loss:0.03673939034342766\n",
      "Epoch:10 Iteration:2300 Loss:0.048247165977954865\n",
      "Epoch:10 Iteration:2400 Loss:0.03837496042251587\n",
      "Epoch:10 Test Loss:0.06543583422899246, Train R^2:0.9511155486106873, Test R^2:0.9405199289321899\n",
      "Epoch:11 Iteration:100 Loss:0.04681023210287094\n",
      "Epoch:11 Iteration:200 Loss:0.042650699615478516\n",
      "Epoch:11 Iteration:300 Loss:0.03788955509662628\n",
      "Epoch:11 Iteration:400 Loss:0.04246752709150314\n",
      "Epoch:11 Iteration:500 Loss:0.02708664909005165\n",
      "Epoch:11 Iteration:600 Loss:0.03293531388044357\n",
      "Epoch:11 Iteration:700 Loss:0.042724207043647766\n",
      "Epoch:11 Iteration:800 Loss:0.03092384710907936\n",
      "Epoch:11 Iteration:900 Loss:0.035657674074172974\n",
      "Epoch:11 Iteration:1000 Loss:0.08662620186805725\n",
      "Epoch:11 Iteration:1100 Loss:0.04209120199084282\n",
      "Epoch:11 Iteration:1200 Loss:0.044382911175489426\n",
      "Epoch:11 Iteration:1300 Loss:0.06211024150252342\n",
      "Epoch:11 Iteration:1400 Loss:0.05928153544664383\n",
      "Epoch:11 Iteration:1500 Loss:0.05204439535737038\n",
      "Epoch:11 Iteration:1600 Loss:0.03229789808392525\n",
      "Epoch:11 Iteration:1700 Loss:0.07204544544219971\n",
      "Epoch:11 Iteration:1800 Loss:0.03889980912208557\n",
      "Epoch:11 Iteration:1900 Loss:0.048014454543590546\n",
      "Epoch:11 Iteration:2000 Loss:0.04880741611123085\n",
      "Epoch:11 Iteration:2100 Loss:0.0523746982216835\n",
      "Epoch:11 Iteration:2200 Loss:0.037504974752664566\n",
      "Epoch:11 Iteration:2300 Loss:0.14287728071212769\n",
      "Epoch:11 Iteration:2400 Loss:0.041686996817588806\n",
      "Epoch:11 Test Loss:0.0757279098033905, Train R^2:0.9525841474533081, Test R^2:0.9311832785606384\n",
      "Epoch:12 Iteration:100 Loss:0.0234023816883564\n",
      "Epoch:12 Iteration:200 Loss:0.02383996732532978\n",
      "Epoch:12 Iteration:300 Loss:0.032492928206920624\n",
      "Epoch:12 Iteration:400 Loss:0.028451766818761826\n",
      "Epoch:12 Iteration:500 Loss:0.049422260373830795\n",
      "Epoch:12 Iteration:600 Loss:0.03357445076107979\n",
      "Epoch:12 Iteration:700 Loss:0.026812491938471794\n",
      "Epoch:12 Iteration:800 Loss:0.07461950927972794\n",
      "Epoch:12 Iteration:900 Loss:0.04649954289197922\n",
      "Epoch:12 Iteration:1000 Loss:0.02668558806180954\n",
      "Epoch:12 Iteration:1100 Loss:0.026376502588391304\n",
      "Epoch:12 Iteration:1200 Loss:0.06207507476210594\n",
      "Epoch:12 Iteration:1300 Loss:0.08683477342128754\n",
      "Epoch:12 Iteration:1400 Loss:0.025499887764453888\n",
      "Epoch:12 Iteration:1500 Loss:0.07048369199037552\n",
      "Epoch:12 Iteration:1600 Loss:0.06234337389469147\n",
      "Epoch:12 Iteration:1700 Loss:0.03199290484189987\n",
      "Epoch:12 Iteration:1800 Loss:0.07936421781778336\n",
      "Epoch:12 Iteration:1900 Loss:0.024816278368234634\n",
      "Epoch:12 Iteration:2000 Loss:0.03132143244147301\n",
      "Epoch:12 Iteration:2100 Loss:0.05600395053625107\n",
      "Epoch:12 Iteration:2200 Loss:0.06329560279846191\n",
      "Epoch:12 Iteration:2300 Loss:0.06077592819929123\n",
      "Epoch:12 Iteration:2400 Loss:0.043768320232629776\n",
      "Epoch:12 Test Loss:0.07497099041938782, Train R^2:0.9545850157737732, Test R^2:0.9318581223487854\n",
      "Epoch:13 Iteration:100 Loss:0.039173856377601624\n",
      "Epoch:13 Iteration:200 Loss:0.03496808186173439\n",
      "Epoch:13 Iteration:300 Loss:0.03499482572078705\n",
      "Epoch:13 Iteration:400 Loss:0.056364886462688446\n",
      "Epoch:13 Iteration:500 Loss:0.06527664512395859\n",
      "Epoch:13 Iteration:600 Loss:0.023671817034482956\n",
      "Epoch:13 Iteration:700 Loss:0.06561516225337982\n",
      "Epoch:13 Iteration:800 Loss:0.017129868268966675\n",
      "Epoch:13 Iteration:900 Loss:0.06757405400276184\n",
      "Epoch:13 Iteration:1000 Loss:0.026535136625170708\n",
      "Epoch:13 Iteration:1100 Loss:0.028727570548653603\n",
      "Epoch:13 Iteration:1200 Loss:0.03652029111981392\n",
      "Epoch:13 Iteration:1300 Loss:0.025407705456018448\n",
      "Epoch:13 Iteration:1400 Loss:0.04327424243092537\n",
      "Epoch:13 Iteration:1500 Loss:0.04071715474128723\n",
      "Epoch:13 Iteration:1600 Loss:0.11898770928382874\n",
      "Epoch:13 Iteration:1700 Loss:0.02694682963192463\n",
      "Epoch:13 Iteration:1800 Loss:0.030852515250444412\n",
      "Epoch:13 Iteration:1900 Loss:0.026195479556918144\n",
      "Epoch:13 Iteration:2000 Loss:0.03216693922877312\n",
      "Epoch:13 Iteration:2100 Loss:0.07267467677593231\n",
      "Epoch:13 Iteration:2200 Loss:0.04103627800941467\n",
      "Epoch:13 Iteration:2300 Loss:0.07137100398540497\n",
      "Epoch:13 Iteration:2400 Loss:0.03786785155534744\n",
      "Epoch:13 Test Loss:0.07079292088747025, Train R^2:0.956229031085968, Test R^2:0.935681939125061\n",
      "Epoch:14 Iteration:100 Loss:0.05692410469055176\n",
      "Epoch:14 Iteration:200 Loss:0.031006334349513054\n",
      "Epoch:14 Iteration:300 Loss:0.039156608283519745\n",
      "Epoch:14 Iteration:400 Loss:0.04058999940752983\n",
      "Epoch:14 Iteration:500 Loss:0.0419350266456604\n",
      "Epoch:14 Iteration:600 Loss:0.06784667074680328\n",
      "Epoch:14 Iteration:700 Loss:0.03526663780212402\n",
      "Epoch:14 Iteration:800 Loss:0.06968903541564941\n",
      "Epoch:14 Iteration:900 Loss:0.052741922438144684\n",
      "Epoch:14 Iteration:1000 Loss:0.0631588026881218\n",
      "Epoch:14 Iteration:1100 Loss:0.0624876394867897\n",
      "Epoch:14 Iteration:1200 Loss:0.04836045578122139\n",
      "Epoch:14 Iteration:1300 Loss:0.034942787140607834\n",
      "Epoch:14 Iteration:1400 Loss:0.0508420467376709\n",
      "Epoch:14 Iteration:1500 Loss:0.0406346470117569\n",
      "Epoch:14 Iteration:1600 Loss:0.05186816304922104\n",
      "Epoch:14 Iteration:1700 Loss:0.036391567438840866\n",
      "Epoch:14 Iteration:1800 Loss:0.057483963668346405\n",
      "Epoch:14 Iteration:1900 Loss:0.030095309019088745\n",
      "Epoch:14 Iteration:2000 Loss:0.04072938486933708\n",
      "Epoch:14 Iteration:2100 Loss:0.06467970460653305\n",
      "Epoch:14 Iteration:2200 Loss:0.02658565156161785\n",
      "Epoch:14 Iteration:2300 Loss:0.042325206100940704\n",
      "Epoch:14 Iteration:2400 Loss:0.05087433382868767\n",
      "Epoch:14 Test Loss:0.06842641532421112, Train R^2:0.9577215909957886, Test R^2:0.9378172159194946\n",
      "Epoch:15 Iteration:100 Loss:0.03830602392554283\n",
      "Epoch:15 Iteration:200 Loss:0.026446102187037468\n",
      "Epoch:15 Iteration:300 Loss:0.0272267647087574\n",
      "Epoch:15 Iteration:400 Loss:0.04096203297376633\n",
      "Epoch:15 Iteration:500 Loss:0.08515926450490952\n",
      "Epoch:15 Iteration:600 Loss:0.0269827488809824\n",
      "Epoch:15 Iteration:700 Loss:0.029284287244081497\n",
      "Epoch:15 Iteration:800 Loss:0.05418853461742401\n",
      "Epoch:15 Iteration:900 Loss:0.028484845533967018\n",
      "Epoch:15 Iteration:1000 Loss:0.03820314258337021\n",
      "Epoch:15 Iteration:1100 Loss:0.03067706525325775\n",
      "Epoch:15 Iteration:1200 Loss:0.03389239311218262\n",
      "Epoch:15 Iteration:1300 Loss:0.04783293604850769\n",
      "Epoch:15 Iteration:1400 Loss:0.03485097736120224\n",
      "Epoch:15 Iteration:1500 Loss:0.05763384699821472\n",
      "Epoch:15 Iteration:1600 Loss:0.051512520760297775\n",
      "Epoch:15 Iteration:1700 Loss:0.034324441105127335\n",
      "Epoch:15 Iteration:1800 Loss:0.03539634868502617\n",
      "Epoch:15 Iteration:1900 Loss:0.03988155722618103\n",
      "Epoch:15 Iteration:2000 Loss:0.031668562442064285\n",
      "Epoch:15 Iteration:2100 Loss:0.03681761771440506\n",
      "Epoch:15 Iteration:2200 Loss:0.046669963747262955\n",
      "Epoch:15 Iteration:2300 Loss:0.07713913917541504\n",
      "Epoch:15 Iteration:2400 Loss:0.021610010415315628\n",
      "Epoch:15 Test Loss:0.07056129723787308, Train R^2:0.959097683429718, Test R^2:0.935865044593811\n",
      "Epoch:16 Iteration:100 Loss:0.055046893656253815\n",
      "Epoch:16 Iteration:200 Loss:0.06013806536793709\n",
      "Epoch:16 Iteration:300 Loss:0.025596696883440018\n",
      "Epoch:16 Iteration:400 Loss:0.02364293485879898\n",
      "Epoch:16 Iteration:500 Loss:0.026656661182641983\n",
      "Epoch:16 Iteration:600 Loss:0.03670179843902588\n",
      "Epoch:16 Iteration:700 Loss:0.030905665829777718\n",
      "Epoch:16 Iteration:800 Loss:0.04778836667537689\n",
      "Epoch:16 Iteration:900 Loss:0.06476499140262604\n",
      "Epoch:16 Iteration:1000 Loss:0.0393826961517334\n",
      "Epoch:16 Iteration:1100 Loss:0.04254689812660217\n",
      "Epoch:16 Iteration:1200 Loss:0.03305026516318321\n",
      "Epoch:16 Iteration:1300 Loss:0.028809327632188797\n",
      "Epoch:16 Iteration:1400 Loss:0.07019291818141937\n",
      "Epoch:16 Iteration:1500 Loss:0.05059770494699478\n",
      "Epoch:16 Iteration:1600 Loss:0.06582838296890259\n",
      "Epoch:16 Iteration:1700 Loss:0.040236860513687134\n",
      "Epoch:16 Iteration:1800 Loss:0.03146406263113022\n",
      "Epoch:16 Iteration:1900 Loss:0.06741196662187576\n",
      "Epoch:16 Iteration:2000 Loss:0.0477314293384552\n",
      "Epoch:16 Iteration:2100 Loss:0.03764517605304718\n",
      "Epoch:16 Iteration:2200 Loss:0.054009657353162766\n",
      "Epoch:16 Iteration:2300 Loss:0.0642693042755127\n",
      "Epoch:16 Iteration:2400 Loss:0.04215307533740997\n",
      "Epoch:16 Test Loss:0.07529470324516296, Train R^2:0.9600311517715454, Test R^2:0.9315715432167053\n",
      "Epoch:17 Iteration:100 Loss:0.023831656202673912\n",
      "Epoch:17 Iteration:200 Loss:0.031851813197135925\n",
      "Epoch:17 Iteration:300 Loss:0.028573647141456604\n",
      "Epoch:17 Iteration:400 Loss:0.02805135026574135\n",
      "Epoch:17 Iteration:500 Loss:0.04947984963655472\n",
      "Epoch:17 Iteration:600 Loss:0.028792403638362885\n",
      "Epoch:17 Iteration:700 Loss:0.023296484723687172\n",
      "Epoch:17 Iteration:800 Loss:0.029643313959240913\n",
      "Epoch:17 Iteration:900 Loss:0.01429149229079485\n",
      "Epoch:17 Iteration:1000 Loss:0.033456951379776\n",
      "Epoch:17 Iteration:1100 Loss:0.050584908574819565\n",
      "Epoch:17 Iteration:1200 Loss:0.04700106009840965\n",
      "Epoch:17 Iteration:1300 Loss:0.05700415372848511\n",
      "Epoch:17 Iteration:1400 Loss:0.030402999371290207\n",
      "Epoch:17 Iteration:1500 Loss:0.03798709437251091\n",
      "Epoch:17 Iteration:1600 Loss:0.040770843625068665\n",
      "Epoch:17 Iteration:1700 Loss:0.031190544366836548\n",
      "Epoch:17 Iteration:1800 Loss:0.06684662401676178\n",
      "Epoch:17 Iteration:1900 Loss:0.0340762585401535\n",
      "Epoch:17 Iteration:2000 Loss:0.03465999662876129\n",
      "Epoch:17 Iteration:2100 Loss:0.03857389837503433\n",
      "Epoch:17 Iteration:2200 Loss:0.05601641163229942\n",
      "Epoch:17 Iteration:2300 Loss:0.026342179626226425\n",
      "Epoch:17 Iteration:2400 Loss:0.039284322410821915\n",
      "Epoch:17 Test Loss:0.07088601589202881, Train R^2:0.9611814022064209, Test R^2:0.9355732202529907\n",
      "Epoch:18 Iteration:100 Loss:0.06371453404426575\n",
      "Epoch:18 Iteration:200 Loss:0.0570049062371254\n",
      "Epoch:18 Iteration:300 Loss:0.09430667757987976\n",
      "Epoch:18 Iteration:400 Loss:0.033182479441165924\n",
      "Epoch:18 Iteration:500 Loss:0.04198930040001869\n",
      "Epoch:18 Iteration:600 Loss:0.038143157958984375\n",
      "Epoch:18 Iteration:700 Loss:0.0324786975979805\n",
      "Epoch:18 Iteration:800 Loss:0.026697233319282532\n",
      "Epoch:18 Iteration:900 Loss:0.029182884842157364\n",
      "Epoch:18 Iteration:1000 Loss:0.07048804312944412\n",
      "Epoch:18 Iteration:1100 Loss:0.052191123366355896\n",
      "Epoch:18 Iteration:1200 Loss:0.01421812828630209\n",
      "Epoch:18 Iteration:1300 Loss:0.026740480214357376\n",
      "Epoch:18 Iteration:1400 Loss:0.036904990673065186\n",
      "Epoch:18 Iteration:1500 Loss:0.04678445681929588\n",
      "Epoch:18 Iteration:1600 Loss:0.03197850286960602\n",
      "Epoch:18 Iteration:1700 Loss:0.023518193513154984\n",
      "Epoch:18 Iteration:1800 Loss:0.03464319929480553\n",
      "Epoch:18 Iteration:1900 Loss:0.02222314476966858\n",
      "Epoch:18 Iteration:2000 Loss:0.03057919628918171\n",
      "Epoch:18 Iteration:2100 Loss:0.05578999221324921\n",
      "Epoch:18 Iteration:2200 Loss:0.04041909798979759\n",
      "Epoch:18 Iteration:2300 Loss:0.04614022746682167\n",
      "Epoch:18 Iteration:2400 Loss:0.04822003096342087\n",
      "Epoch:18 Test Loss:0.06675171852111816, Train R^2:0.9622108936309814, Test R^2:0.9393394589424133\n",
      "Epoch:19 Iteration:100 Loss:0.029027454555034637\n",
      "Epoch:19 Iteration:200 Loss:0.05088426545262337\n",
      "Epoch:19 Iteration:300 Loss:0.02422865852713585\n",
      "Epoch:19 Iteration:400 Loss:0.01981290616095066\n",
      "Epoch:19 Iteration:500 Loss:0.039132922887802124\n",
      "Epoch:19 Iteration:600 Loss:0.02461155876517296\n",
      "Epoch:19 Iteration:700 Loss:0.020443500950932503\n",
      "Epoch:19 Iteration:800 Loss:0.05952794477343559\n",
      "Epoch:19 Iteration:900 Loss:0.050284139811992645\n",
      "Epoch:19 Iteration:1000 Loss:0.04187290370464325\n",
      "Epoch:19 Iteration:1100 Loss:0.07696134597063065\n",
      "Epoch:19 Iteration:1200 Loss:0.026195846498012543\n",
      "Epoch:19 Iteration:1300 Loss:0.08660587668418884\n",
      "Epoch:19 Iteration:1400 Loss:0.055576764047145844\n",
      "Epoch:19 Iteration:1500 Loss:0.035763442516326904\n",
      "Epoch:19 Iteration:1600 Loss:0.021304689347743988\n",
      "Epoch:19 Iteration:1700 Loss:0.02884814888238907\n",
      "Epoch:19 Iteration:1800 Loss:0.019713224843144417\n",
      "Epoch:19 Iteration:1900 Loss:0.03432615101337433\n",
      "Epoch:19 Iteration:2000 Loss:0.04891086369752884\n",
      "Epoch:19 Iteration:2100 Loss:0.04440142959356308\n",
      "Epoch:19 Iteration:2200 Loss:0.05931369960308075\n",
      "Epoch:19 Iteration:2300 Loss:0.022185910493135452\n",
      "Epoch:19 Iteration:2400 Loss:0.04917315021157265\n",
      "Epoch:19 Test Loss:0.06847985088825226, Train R^2:0.9633784294128418, Test R^2:0.9377682209014893\n",
      "Epoch:20 Iteration:100 Loss:0.038359396159648895\n",
      "Epoch:20 Iteration:200 Loss:0.023213911801576614\n",
      "Epoch:20 Iteration:300 Loss:0.03203409165143967\n",
      "Epoch:20 Iteration:400 Loss:0.06441374123096466\n",
      "Epoch:20 Iteration:500 Loss:0.017786415293812752\n",
      "Epoch:20 Iteration:600 Loss:0.023656409233808517\n",
      "Epoch:20 Iteration:700 Loss:0.02381613850593567\n",
      "Epoch:20 Iteration:800 Loss:0.031293824315071106\n",
      "Epoch:20 Iteration:900 Loss:0.03972184285521507\n",
      "Epoch:20 Iteration:1000 Loss:0.05157550424337387\n",
      "Epoch:20 Iteration:1100 Loss:0.030407708138227463\n",
      "Epoch:20 Iteration:1200 Loss:0.0454757958650589\n",
      "Epoch:20 Iteration:1300 Loss:0.027752593159675598\n",
      "Epoch:20 Iteration:1400 Loss:0.04673326760530472\n",
      "Epoch:20 Iteration:1500 Loss:0.04406479746103287\n",
      "Epoch:20 Iteration:1600 Loss:0.031954195350408554\n",
      "Epoch:20 Iteration:1700 Loss:0.060546889901161194\n",
      "Epoch:20 Iteration:1800 Loss:0.02982262894511223\n",
      "Epoch:20 Iteration:1900 Loss:0.042141787707805634\n",
      "Epoch:20 Iteration:2000 Loss:0.03320144861936569\n",
      "Epoch:20 Iteration:2100 Loss:0.026430726051330566\n",
      "Epoch:20 Iteration:2200 Loss:0.031406648457050323\n",
      "Epoch:20 Iteration:2300 Loss:0.027626821771264076\n",
      "Epoch:20 Iteration:2400 Loss:0.029969271272420883\n",
      "Epoch:20 Test Loss:0.0681401863694191, Train R^2:0.9641345739364624, Test R^2:0.9380937814712524\n",
      "Epoch:21 Iteration:100 Loss:0.04088526591658592\n",
      "Epoch:21 Iteration:200 Loss:0.027162738144397736\n",
      "Epoch:21 Iteration:300 Loss:0.03995026275515556\n",
      "Epoch:21 Iteration:400 Loss:0.02909848466515541\n",
      "Epoch:21 Iteration:500 Loss:0.02585572376847267\n",
      "Epoch:21 Iteration:600 Loss:0.05326435714960098\n",
      "Epoch:21 Iteration:700 Loss:0.034515008330345154\n",
      "Epoch:21 Iteration:800 Loss:0.038060396909713745\n",
      "Epoch:21 Iteration:900 Loss:0.03276735916733742\n",
      "Epoch:21 Iteration:1000 Loss:0.031365107744932175\n",
      "Epoch:21 Iteration:1100 Loss:0.0416768379509449\n",
      "Epoch:21 Iteration:1200 Loss:0.019234124571084976\n",
      "Epoch:21 Iteration:1300 Loss:0.024213258177042007\n",
      "Epoch:21 Iteration:1400 Loss:0.028999928385019302\n",
      "Epoch:21 Iteration:1500 Loss:0.04594255983829498\n",
      "Epoch:21 Iteration:1600 Loss:0.025779424235224724\n",
      "Epoch:21 Iteration:1700 Loss:0.0355522520840168\n",
      "Epoch:21 Iteration:1800 Loss:0.025301434099674225\n",
      "Epoch:21 Iteration:1900 Loss:0.03208332136273384\n",
      "Epoch:21 Iteration:2000 Loss:0.02278919145464897\n",
      "Epoch:21 Iteration:2100 Loss:0.03818855434656143\n",
      "Epoch:21 Iteration:2200 Loss:0.03793542832136154\n",
      "Epoch:21 Iteration:2300 Loss:0.04031664878129959\n",
      "Epoch:21 Iteration:2400 Loss:0.058500759303569794\n",
      "Epoch:21 Test Loss:0.07478681951761246, Train R^2:0.965890645980835, Test R^2:0.9320504665374756\n",
      "Epoch:22 Iteration:100 Loss:0.027735572308301926\n",
      "Epoch:22 Iteration:200 Loss:0.026512984186410904\n",
      "Epoch:22 Iteration:300 Loss:0.02166948840022087\n",
      "Epoch:22 Iteration:400 Loss:0.03299659118056297\n",
      "Epoch:22 Iteration:500 Loss:0.07585063576698303\n",
      "Epoch:22 Iteration:600 Loss:0.031370166689157486\n",
      "Epoch:22 Iteration:700 Loss:0.024696439504623413\n",
      "Epoch:22 Iteration:800 Loss:0.03498995676636696\n",
      "Epoch:22 Iteration:900 Loss:0.03539165109395981\n",
      "Epoch:22 Iteration:1000 Loss:0.04152955487370491\n",
      "Epoch:22 Iteration:1100 Loss:0.03586050495505333\n",
      "Epoch:22 Iteration:1200 Loss:0.04686770588159561\n",
      "Epoch:22 Iteration:1300 Loss:0.031472206115722656\n",
      "Epoch:22 Iteration:1400 Loss:0.03283175081014633\n",
      "Epoch:22 Iteration:1500 Loss:0.026619454845786095\n",
      "Epoch:22 Iteration:1600 Loss:0.04751349985599518\n",
      "Epoch:22 Iteration:1700 Loss:0.08072219789028168\n",
      "Epoch:22 Iteration:1800 Loss:0.03600076586008072\n",
      "Epoch:22 Iteration:1900 Loss:0.05401166155934334\n",
      "Epoch:22 Iteration:2000 Loss:0.03268780559301376\n",
      "Epoch:22 Iteration:2100 Loss:0.020163198933005333\n",
      "Epoch:22 Iteration:2200 Loss:0.02245967462658882\n",
      "Epoch:22 Iteration:2300 Loss:0.027623670175671577\n",
      "Epoch:22 Iteration:2400 Loss:0.03709162771701813\n",
      "Epoch:22 Test Loss:0.07289779186248779, Train R^2:0.966219425201416, Test R^2:0.9337484240531921\n",
      "Epoch:23 Iteration:100 Loss:0.055904269218444824\n",
      "Epoch:23 Iteration:200 Loss:0.04455830156803131\n",
      "Epoch:23 Iteration:300 Loss:0.024708593264222145\n",
      "Epoch:23 Iteration:400 Loss:0.02329472452402115\n",
      "Epoch:23 Iteration:500 Loss:0.020247861742973328\n",
      "Epoch:23 Iteration:600 Loss:0.03865767642855644\n",
      "Epoch:23 Iteration:700 Loss:0.023602113127708435\n",
      "Epoch:23 Iteration:800 Loss:0.01926330104470253\n",
      "Epoch:23 Iteration:900 Loss:0.0592346265912056\n",
      "Epoch:23 Iteration:1000 Loss:0.03949514031410217\n",
      "Epoch:23 Iteration:1100 Loss:0.014473961666226387\n",
      "Epoch:23 Iteration:1200 Loss:0.03230005502700806\n",
      "Epoch:23 Iteration:1300 Loss:0.03723491355776787\n",
      "Epoch:23 Iteration:1400 Loss:0.038334790617227554\n",
      "Epoch:23 Iteration:1500 Loss:0.03166245296597481\n",
      "Epoch:23 Iteration:1600 Loss:0.04190082848072052\n",
      "Epoch:23 Iteration:1700 Loss:0.04574655741453171\n",
      "Epoch:23 Iteration:1800 Loss:0.06775575876235962\n",
      "Epoch:23 Iteration:1900 Loss:0.017910044640302658\n",
      "Epoch:23 Iteration:2000 Loss:0.046416379511356354\n",
      "Epoch:23 Iteration:2100 Loss:0.0366552472114563\n",
      "Epoch:23 Iteration:2200 Loss:0.02859502285718918\n",
      "Epoch:23 Iteration:2300 Loss:0.04700912535190582\n",
      "Epoch:23 Iteration:2400 Loss:0.07666340470314026\n",
      "Epoch:23 Test Loss:0.06910692155361176, Train R^2:0.9670253396034241, Test R^2:0.937184751033783\n",
      "Epoch:24 Iteration:100 Loss:0.021116575226187706\n",
      "Epoch:24 Iteration:200 Loss:0.027723778039216995\n",
      "Epoch:24 Iteration:300 Loss:0.06943393498659134\n",
      "Epoch:24 Iteration:400 Loss:0.018656443804502487\n",
      "Epoch:24 Iteration:500 Loss:0.03008343279361725\n",
      "Epoch:24 Iteration:600 Loss:0.028199680149555206\n",
      "Epoch:24 Iteration:700 Loss:0.03158126026391983\n",
      "Epoch:24 Iteration:800 Loss:0.049117594957351685\n",
      "Epoch:24 Iteration:900 Loss:0.043720975518226624\n",
      "Epoch:24 Iteration:1000 Loss:0.03718400001525879\n",
      "Epoch:24 Iteration:1100 Loss:0.030152516439557076\n",
      "Epoch:24 Iteration:1200 Loss:0.020004644989967346\n",
      "Epoch:24 Iteration:1300 Loss:0.024165228009223938\n",
      "Epoch:24 Iteration:1400 Loss:0.019366849213838577\n",
      "Epoch:24 Iteration:1500 Loss:0.02462337538599968\n",
      "Epoch:24 Iteration:1600 Loss:0.04658743739128113\n",
      "Epoch:24 Iteration:1700 Loss:0.02531217783689499\n",
      "Epoch:24 Iteration:1800 Loss:0.022673483937978745\n",
      "Epoch:24 Iteration:1900 Loss:0.03076130524277687\n",
      "Epoch:24 Iteration:2000 Loss:0.02212338149547577\n",
      "Epoch:24 Iteration:2100 Loss:0.020572219043970108\n",
      "Epoch:24 Iteration:2200 Loss:0.02447357214987278\n",
      "Epoch:24 Iteration:2300 Loss:0.03090520203113556\n",
      "Epoch:24 Iteration:2400 Loss:0.023996125906705856\n",
      "Epoch:24 Test Loss:0.07394413650035858, Train R^2:0.9679115414619446, Test R^2:0.9328094124794006\n",
      "Epoch:25 Iteration:100 Loss:0.023856520652770996\n",
      "Epoch:25 Iteration:200 Loss:0.01944464072585106\n",
      "Epoch:25 Iteration:300 Loss:0.1844593584537506\n",
      "Epoch:25 Iteration:400 Loss:0.026658691465854645\n",
      "Epoch:25 Iteration:500 Loss:0.03128747642040253\n",
      "Epoch:25 Iteration:600 Loss:0.02826423943042755\n",
      "Epoch:25 Iteration:700 Loss:0.020541507750749588\n",
      "Epoch:25 Iteration:800 Loss:0.012131897732615471\n",
      "Epoch:25 Iteration:900 Loss:0.031104732304811478\n",
      "Epoch:25 Iteration:1000 Loss:0.03153586387634277\n",
      "Epoch:25 Iteration:1100 Loss:0.023751363158226013\n",
      "Epoch:25 Iteration:1200 Loss:0.03125552833080292\n",
      "Epoch:25 Iteration:1300 Loss:0.029988029971718788\n",
      "Epoch:25 Iteration:1400 Loss:0.0290080513805151\n",
      "Epoch:25 Iteration:1500 Loss:0.05139052867889404\n",
      "Epoch:25 Iteration:1600 Loss:0.026890195906162262\n",
      "Epoch:25 Iteration:1700 Loss:0.044063784182071686\n",
      "Epoch:25 Iteration:1800 Loss:0.07052551209926605\n",
      "Epoch:25 Iteration:1900 Loss:0.019193019717931747\n",
      "Epoch:25 Iteration:2000 Loss:0.02198765054345131\n",
      "Epoch:25 Iteration:2100 Loss:0.03526812419295311\n",
      "Epoch:25 Iteration:2200 Loss:0.03578285872936249\n",
      "Epoch:25 Iteration:2300 Loss:0.0477127805352211\n",
      "Epoch:25 Iteration:2400 Loss:0.032379381358623505\n",
      "Epoch:25 Test Loss:0.07201188057661057, Train R^2:0.968967080116272, Test R^2:0.9345630407333374\n",
      "Epoch:26 Iteration:100 Loss:0.022759828716516495\n",
      "Epoch:26 Iteration:200 Loss:0.03139301761984825\n",
      "Epoch:26 Iteration:300 Loss:0.011383067816495895\n",
      "Epoch:26 Iteration:400 Loss:0.03362412750720978\n",
      "Epoch:26 Iteration:500 Loss:0.01958460360765457\n",
      "Epoch:26 Iteration:600 Loss:0.07890266925096512\n",
      "Epoch:26 Iteration:700 Loss:0.019144108518958092\n",
      "Epoch:26 Iteration:800 Loss:0.042200274765491486\n",
      "Epoch:26 Iteration:900 Loss:0.05107954889535904\n",
      "Epoch:26 Iteration:1000 Loss:0.04180718585848808\n",
      "Epoch:26 Iteration:1100 Loss:0.028890397399663925\n",
      "Epoch:26 Iteration:1200 Loss:0.059800807386636734\n",
      "Epoch:26 Iteration:1300 Loss:0.0364585779607296\n",
      "Epoch:26 Iteration:1400 Loss:0.045278698205947876\n",
      "Epoch:26 Iteration:1500 Loss:0.032794367522001266\n",
      "Epoch:26 Iteration:1600 Loss:0.02872941642999649\n",
      "Epoch:26 Iteration:1700 Loss:0.028211478143930435\n",
      "Epoch:26 Iteration:1800 Loss:0.022466590628027916\n",
      "Epoch:26 Iteration:1900 Loss:0.03104551136493683\n",
      "Epoch:26 Iteration:2000 Loss:0.025365842506289482\n",
      "Epoch:26 Iteration:2100 Loss:0.0262041836977005\n",
      "Epoch:26 Iteration:2200 Loss:0.028378929942846298\n",
      "Epoch:26 Iteration:2300 Loss:0.03269730880856514\n",
      "Epoch:26 Iteration:2400 Loss:0.04498709365725517\n",
      "Epoch:26 Test Loss:0.0744808092713356, Train R^2:0.9692007303237915, Test R^2:0.932368278503418\n",
      "Epoch:27 Iteration:100 Loss:0.03199043124914169\n",
      "Epoch:27 Iteration:200 Loss:0.016322124749422073\n",
      "Epoch:27 Iteration:300 Loss:0.060181908309459686\n",
      "Epoch:27 Iteration:400 Loss:0.04392089322209358\n",
      "Epoch:27 Iteration:500 Loss:0.03175431862473488\n",
      "Epoch:27 Iteration:600 Loss:0.028246339410543442\n",
      "Epoch:27 Iteration:700 Loss:0.03868617117404938\n",
      "Epoch:27 Iteration:800 Loss:0.05745095759630203\n",
      "Epoch:27 Iteration:900 Loss:0.05036529526114464\n",
      "Epoch:27 Iteration:1000 Loss:0.12002667039632797\n",
      "Epoch:27 Iteration:1100 Loss:0.026731979101896286\n",
      "Epoch:27 Iteration:1200 Loss:0.1266520619392395\n",
      "Epoch:27 Iteration:1300 Loss:0.023416759446263313\n",
      "Epoch:27 Iteration:1400 Loss:0.03301941230893135\n",
      "Epoch:27 Iteration:1500 Loss:0.030654700472950935\n",
      "Epoch:27 Iteration:1600 Loss:0.03378114104270935\n",
      "Epoch:27 Iteration:1700 Loss:0.04132365062832832\n",
      "Epoch:27 Iteration:1800 Loss:0.025577260181307793\n",
      "Epoch:27 Iteration:1900 Loss:0.023307189345359802\n",
      "Epoch:27 Iteration:2000 Loss:0.033799462020397186\n",
      "Epoch:27 Iteration:2100 Loss:0.02804413251578808\n",
      "Epoch:27 Iteration:2200 Loss:0.13668204843997955\n",
      "Epoch:27 Iteration:2300 Loss:0.02376691624522209\n",
      "Epoch:27 Iteration:2400 Loss:0.023311780765652657\n",
      "Epoch:27 Test Loss:0.0721224695444107, Train R^2:0.970140278339386, Test R^2:0.934463381767273\n",
      "Epoch:28 Iteration:100 Loss:0.04962022602558136\n",
      "Epoch:28 Iteration:200 Loss:0.02350197359919548\n",
      "Epoch:28 Iteration:300 Loss:0.03985592722892761\n",
      "Epoch:28 Iteration:400 Loss:0.024094536900520325\n",
      "Epoch:28 Iteration:500 Loss:0.028737526386976242\n",
      "Epoch:28 Iteration:600 Loss:0.02470639906823635\n",
      "Epoch:28 Iteration:700 Loss:0.02738194912672043\n",
      "Epoch:28 Iteration:800 Loss:0.030918467789888382\n",
      "Epoch:28 Iteration:900 Loss:0.022430021315813065\n",
      "Epoch:28 Iteration:1000 Loss:0.027098145335912704\n",
      "Epoch:28 Iteration:1100 Loss:0.03246741741895676\n",
      "Epoch:28 Iteration:1200 Loss:0.028099915012717247\n",
      "Epoch:28 Iteration:1300 Loss:0.03298795223236084\n",
      "Epoch:28 Iteration:1400 Loss:0.03382405266165733\n",
      "Epoch:28 Iteration:1500 Loss:0.024438217282295227\n",
      "Epoch:28 Iteration:1600 Loss:0.033830419182777405\n",
      "Epoch:28 Iteration:1700 Loss:0.030373692512512207\n",
      "Epoch:28 Iteration:1800 Loss:0.05995887145400047\n",
      "Epoch:28 Iteration:1900 Loss:0.03316125273704529\n",
      "Epoch:28 Iteration:2000 Loss:0.02997947297990322\n",
      "Epoch:28 Iteration:2100 Loss:0.05180510878562927\n",
      "Epoch:28 Iteration:2200 Loss:0.029355881735682487\n",
      "Epoch:28 Iteration:2300 Loss:0.028732575476169586\n",
      "Epoch:28 Iteration:2400 Loss:0.021367616951465607\n",
      "Epoch:28 Test Loss:0.07227551192045212, Train R^2:0.9709275364875793, Test R^2:0.9343148469924927\n",
      "Epoch:29 Iteration:100 Loss:0.028304655104875565\n",
      "Epoch:29 Iteration:200 Loss:0.02931237779557705\n",
      "Epoch:29 Iteration:300 Loss:0.03091948851943016\n",
      "Epoch:29 Iteration:400 Loss:0.012825780548155308\n",
      "Epoch:29 Iteration:500 Loss:0.029680471867322922\n",
      "Epoch:29 Iteration:600 Loss:0.02047669142484665\n",
      "Epoch:29 Iteration:700 Loss:0.025987062603235245\n",
      "Epoch:29 Iteration:800 Loss:0.035465434193611145\n",
      "Epoch:29 Iteration:900 Loss:0.01564682275056839\n",
      "Epoch:29 Iteration:1000 Loss:0.029123060405254364\n",
      "Epoch:29 Iteration:1100 Loss:0.0301370769739151\n",
      "Epoch:29 Iteration:1200 Loss:0.029336905106902122\n",
      "Epoch:29 Iteration:1300 Loss:0.03841707110404968\n",
      "Epoch:29 Iteration:1400 Loss:0.04641401022672653\n",
      "Epoch:29 Iteration:1500 Loss:0.018646694719791412\n",
      "Epoch:29 Iteration:1600 Loss:0.03923003375530243\n",
      "Epoch:29 Iteration:1700 Loss:0.02871491014957428\n",
      "Epoch:29 Iteration:1800 Loss:0.04048715531826019\n",
      "Epoch:29 Iteration:1900 Loss:0.026250574737787247\n",
      "Epoch:29 Iteration:2000 Loss:0.050441209226846695\n",
      "Epoch:29 Iteration:2100 Loss:0.018886180594563484\n",
      "Epoch:29 Iteration:2200 Loss:0.028924766927957535\n",
      "Epoch:29 Iteration:2300 Loss:0.05022033676505089\n",
      "Epoch:29 Iteration:2400 Loss:0.020773272961378098\n",
      "Epoch:29 Test Loss:0.0708978995680809, Train R^2:0.9696424603462219, Test R^2:0.9355824589729309\n",
      "Epoch:30 Iteration:100 Loss:0.054815568029880524\n",
      "Epoch:30 Iteration:200 Loss:0.021831996738910675\n",
      "Epoch:30 Iteration:300 Loss:0.015153774060308933\n",
      "Epoch:30 Iteration:400 Loss:0.07673251628875732\n",
      "Epoch:30 Iteration:500 Loss:0.02966504544019699\n",
      "Epoch:30 Iteration:600 Loss:0.024769369512796402\n",
      "Epoch:30 Iteration:700 Loss:0.05948294699192047\n",
      "Epoch:30 Iteration:800 Loss:0.03747161850333214\n",
      "Epoch:30 Iteration:900 Loss:0.017842279747128487\n",
      "Epoch:30 Iteration:1000 Loss:0.03356648609042168\n",
      "Epoch:30 Iteration:1100 Loss:0.046036459505558014\n",
      "Epoch:30 Iteration:1200 Loss:0.03844761103391647\n",
      "Epoch:30 Iteration:1300 Loss:0.025908421725034714\n",
      "Epoch:30 Iteration:1400 Loss:0.03957032784819603\n",
      "Epoch:30 Iteration:1500 Loss:0.033194057643413544\n",
      "Epoch:30 Iteration:1600 Loss:0.028128456324338913\n",
      "Epoch:30 Iteration:1700 Loss:0.0251636803150177\n",
      "Epoch:30 Iteration:1800 Loss:0.022835806012153625\n",
      "Epoch:30 Iteration:1900 Loss:0.03048931248486042\n",
      "Epoch:30 Iteration:2000 Loss:0.0242463331669569\n",
      "Epoch:30 Iteration:2100 Loss:0.04881729185581207\n",
      "Epoch:30 Iteration:2200 Loss:0.02887289971113205\n",
      "Epoch:30 Iteration:2300 Loss:0.0438409298658371\n",
      "Epoch:30 Iteration:2400 Loss:0.06543571501970291\n",
      "Epoch:30 Test Loss:0.0734713226556778, Train R^2:0.9716223478317261, Test R^2:0.9332267642021179\n",
      "Epoch:31 Iteration:100 Loss:0.02229570783674717\n",
      "Epoch:31 Iteration:200 Loss:0.044327057898044586\n",
      "Epoch:31 Iteration:300 Loss:0.019573785364627838\n",
      "Epoch:31 Iteration:400 Loss:0.016293874010443687\n",
      "Epoch:31 Iteration:500 Loss:0.03433605283498764\n",
      "Epoch:31 Iteration:600 Loss:0.03564035892486572\n",
      "Epoch:31 Iteration:700 Loss:0.03059944324195385\n",
      "Epoch:31 Iteration:800 Loss:0.05044371634721756\n",
      "Epoch:31 Iteration:900 Loss:0.0367446169257164\n",
      "Epoch:31 Iteration:1000 Loss:0.01731366291642189\n",
      "Epoch:31 Iteration:1100 Loss:0.02156740427017212\n",
      "Epoch:31 Iteration:1200 Loss:0.02220657840371132\n",
      "Epoch:31 Iteration:1300 Loss:0.028304260224103928\n",
      "Epoch:31 Iteration:1400 Loss:0.1403837352991104\n",
      "Epoch:31 Iteration:1500 Loss:0.019511748105287552\n",
      "Epoch:31 Iteration:1600 Loss:0.027875566855072975\n",
      "Epoch:31 Iteration:1700 Loss:0.06087782233953476\n",
      "Epoch:31 Iteration:1800 Loss:0.01874774880707264\n",
      "Epoch:31 Iteration:1900 Loss:0.026015758514404297\n",
      "Epoch:31 Iteration:2000 Loss:0.011114981956779957\n",
      "Epoch:31 Iteration:2100 Loss:0.025890201330184937\n",
      "Epoch:31 Iteration:2200 Loss:0.0396246500313282\n",
      "Epoch:31 Iteration:2300 Loss:0.03477037698030472\n",
      "Epoch:31 Iteration:2400 Loss:0.03179800510406494\n",
      "Epoch:31 Test Loss:0.07333686202764511, Train R^2:0.9728855490684509, Test R^2:0.9333499073982239\n",
      "Epoch:32 Iteration:100 Loss:0.02470843866467476\n",
      "Epoch:32 Iteration:200 Loss:0.012107310816645622\n",
      "Epoch:32 Iteration:300 Loss:0.026064347475767136\n",
      "Epoch:32 Iteration:400 Loss:0.013211997225880623\n",
      "Epoch:32 Iteration:500 Loss:0.013035974465310574\n",
      "Epoch:32 Iteration:600 Loss:0.02335280552506447\n",
      "Epoch:32 Iteration:700 Loss:0.026006359606981277\n",
      "Epoch:32 Iteration:800 Loss:0.02096674218773842\n",
      "Epoch:32 Iteration:900 Loss:0.020354129374027252\n",
      "Epoch:32 Iteration:1000 Loss:0.02258457988500595\n",
      "Epoch:32 Iteration:1100 Loss:0.01715783029794693\n",
      "Epoch:32 Iteration:1200 Loss:0.0391007661819458\n",
      "Epoch:32 Iteration:1300 Loss:0.03166278451681137\n",
      "Epoch:32 Iteration:1400 Loss:0.03341725468635559\n",
      "Epoch:32 Iteration:1500 Loss:0.031467244029045105\n",
      "Epoch:32 Iteration:1600 Loss:0.027441803365945816\n",
      "Epoch:32 Iteration:1700 Loss:0.03175504505634308\n",
      "Epoch:32 Iteration:1800 Loss:0.025902308523654938\n",
      "Epoch:32 Iteration:1900 Loss:0.027932969853281975\n",
      "Epoch:32 Iteration:2000 Loss:0.028266146779060364\n",
      "Epoch:32 Iteration:2100 Loss:0.03744905814528465\n",
      "Epoch:32 Iteration:2200 Loss:0.018472792580723763\n",
      "Epoch:32 Iteration:2300 Loss:0.02689066156744957\n",
      "Epoch:32 Iteration:2400 Loss:0.04249060899019241\n",
      "Epoch:32 Test Loss:0.0735296681523323, Train R^2:0.9732736945152283, Test R^2:0.9332109093666077\n",
      "Epoch:33 Iteration:100 Loss:0.033361662179231644\n",
      "Epoch:33 Iteration:200 Loss:0.016477160155773163\n",
      "Epoch:33 Iteration:300 Loss:0.02462054416537285\n",
      "Epoch:33 Iteration:400 Loss:0.05948265269398689\n",
      "Epoch:33 Iteration:500 Loss:0.0325033962726593\n",
      "Epoch:33 Iteration:600 Loss:0.016779303550720215\n",
      "Epoch:33 Iteration:700 Loss:0.015189383178949356\n",
      "Epoch:33 Iteration:800 Loss:0.03937559202313423\n",
      "Epoch:33 Iteration:900 Loss:0.03110244870185852\n",
      "Epoch:33 Iteration:1000 Loss:0.02071569487452507\n",
      "Epoch:33 Iteration:1100 Loss:0.022595643997192383\n",
      "Epoch:33 Iteration:1200 Loss:0.03776705265045166\n",
      "Epoch:33 Iteration:1300 Loss:0.03589332848787308\n",
      "Epoch:33 Iteration:1400 Loss:0.02499501407146454\n",
      "Epoch:33 Iteration:1500 Loss:0.016461899504065514\n",
      "Epoch:33 Iteration:1600 Loss:0.025227438658475876\n",
      "Epoch:33 Iteration:1700 Loss:0.033430203795433044\n",
      "Epoch:33 Iteration:1800 Loss:0.02903607487678528\n",
      "Epoch:33 Iteration:1900 Loss:0.031387925148010254\n",
      "Epoch:33 Iteration:2000 Loss:0.014360655099153519\n",
      "Epoch:33 Iteration:2100 Loss:0.021703220903873444\n",
      "Epoch:33 Iteration:2200 Loss:0.01797756925225258\n",
      "Epoch:33 Iteration:2300 Loss:0.014079449698328972\n",
      "Epoch:33 Iteration:2400 Loss:0.026764201000332832\n",
      "Epoch:33 Test Loss:0.07343771308660507, Train R^2:0.9739099144935608, Test R^2:0.9332633018493652\n",
      "Epoch:34 Iteration:100 Loss:0.019927263259887695\n",
      "Epoch:34 Iteration:200 Loss:0.03014633059501648\n",
      "Epoch:34 Iteration:300 Loss:0.0492522194981575\n",
      "Epoch:34 Iteration:400 Loss:0.020084436982870102\n",
      "Epoch:34 Iteration:500 Loss:0.027556568384170532\n",
      "Epoch:34 Iteration:600 Loss:0.02063089981675148\n",
      "Epoch:34 Iteration:700 Loss:0.01710604317486286\n",
      "Epoch:34 Iteration:800 Loss:0.02064666897058487\n",
      "Epoch:34 Iteration:900 Loss:0.02252565696835518\n",
      "Epoch:34 Iteration:1000 Loss:0.01867852546274662\n",
      "Epoch:34 Iteration:1100 Loss:0.01908026449382305\n",
      "Epoch:34 Iteration:1200 Loss:0.020366882905364037\n",
      "Epoch:34 Iteration:1300 Loss:0.04548588767647743\n",
      "Epoch:34 Iteration:1400 Loss:0.027047425508499146\n",
      "Epoch:34 Iteration:1500 Loss:0.020829958841204643\n",
      "Epoch:34 Iteration:1600 Loss:0.018841663375496864\n",
      "Epoch:34 Iteration:1700 Loss:0.04017909988760948\n",
      "Epoch:34 Iteration:1800 Loss:0.021656354889273643\n",
      "Epoch:34 Iteration:1900 Loss:0.024576639756560326\n",
      "Epoch:34 Iteration:2000 Loss:0.037688981741666794\n",
      "Epoch:34 Iteration:2100 Loss:0.026891497895121574\n",
      "Epoch:34 Iteration:2200 Loss:0.018358061090111732\n",
      "Epoch:34 Iteration:2300 Loss:0.026043329387903214\n",
      "Epoch:34 Iteration:2400 Loss:0.0215307530015707\n",
      "Epoch:34 Test Loss:0.07217547297477722, Train R^2:0.9742474555969238, Test R^2:0.9344428181648254\n",
      "Epoch:35 Iteration:100 Loss:0.015404567122459412\n",
      "Epoch:35 Iteration:200 Loss:0.049647483974695206\n",
      "Epoch:35 Iteration:300 Loss:0.019225772470235825\n",
      "Epoch:35 Iteration:400 Loss:0.03091440163552761\n",
      "Epoch:35 Iteration:500 Loss:0.025254253298044205\n",
      "Epoch:35 Iteration:600 Loss:0.024414001032710075\n",
      "Epoch:35 Iteration:700 Loss:0.015290198847651482\n",
      "Epoch:35 Iteration:800 Loss:0.025125136598944664\n",
      "Epoch:35 Iteration:900 Loss:0.017695261165499687\n",
      "Epoch:35 Iteration:1000 Loss:0.04632365331053734\n",
      "Epoch:35 Iteration:1100 Loss:0.020818088203668594\n",
      "Epoch:35 Iteration:1200 Loss:0.04829847067594528\n",
      "Epoch:35 Iteration:1300 Loss:0.027523964643478394\n",
      "Epoch:35 Iteration:1400 Loss:0.023656006902456284\n",
      "Epoch:35 Iteration:1500 Loss:0.01562780886888504\n",
      "Epoch:35 Iteration:1600 Loss:0.03700053319334984\n",
      "Epoch:35 Iteration:1700 Loss:0.04249401390552521\n",
      "Epoch:35 Iteration:1800 Loss:0.025893043726682663\n",
      "Epoch:35 Iteration:1900 Loss:0.03261788561940193\n",
      "Epoch:35 Iteration:2000 Loss:0.023531777784228325\n",
      "Epoch:35 Iteration:2100 Loss:0.020945308730006218\n",
      "Epoch:35 Iteration:2200 Loss:0.020445609465241432\n",
      "Epoch:35 Iteration:2300 Loss:0.029386315494775772\n",
      "Epoch:35 Iteration:2400 Loss:0.04058133065700531\n",
      "Epoch:35 Test Loss:0.07411999255418777, Train R^2:0.975082278251648, Test R^2:0.9326299428939819\n",
      "Epoch:36 Iteration:100 Loss:0.018706869333982468\n",
      "Epoch:36 Iteration:200 Loss:0.04235033318400383\n",
      "Epoch:36 Iteration:300 Loss:0.01981908455491066\n",
      "Epoch:36 Iteration:400 Loss:0.02867422066628933\n",
      "Epoch:36 Iteration:500 Loss:0.02028130739927292\n",
      "Epoch:36 Iteration:600 Loss:0.013604635372757912\n",
      "Epoch:36 Iteration:700 Loss:0.02010461874306202\n",
      "Epoch:36 Iteration:800 Loss:0.02596655674278736\n",
      "Epoch:36 Iteration:900 Loss:0.017017211765050888\n",
      "Epoch:36 Iteration:1000 Loss:0.017609765753149986\n",
      "Epoch:36 Iteration:1100 Loss:0.02570539154112339\n",
      "Epoch:36 Iteration:1200 Loss:0.02959483489394188\n",
      "Epoch:36 Iteration:1300 Loss:0.02611842192709446\n",
      "Epoch:36 Iteration:1400 Loss:0.009875256568193436\n",
      "Epoch:36 Iteration:1500 Loss:0.034270383417606354\n",
      "Epoch:36 Iteration:1600 Loss:0.021114829927682877\n",
      "Epoch:36 Iteration:1700 Loss:0.03346666321158409\n",
      "Epoch:36 Iteration:1800 Loss:0.02016563154757023\n",
      "Epoch:36 Iteration:1900 Loss:0.019040578976273537\n",
      "Epoch:36 Iteration:2000 Loss:0.02249021828174591\n",
      "Epoch:36 Iteration:2100 Loss:0.03733647242188454\n",
      "Epoch:36 Iteration:2200 Loss:0.011572764255106449\n",
      "Epoch:36 Iteration:2300 Loss:0.03216790035367012\n",
      "Epoch:36 Iteration:2400 Loss:0.03315816447138786\n",
      "Epoch:36 Test Loss:0.07352498918771744, Train R^2:0.975394606590271, Test R^2:0.9331719279289246\n",
      "Epoch:37 Iteration:100 Loss:0.02463485300540924\n",
      "Epoch:37 Iteration:200 Loss:0.013017211109399796\n",
      "Epoch:37 Iteration:300 Loss:0.01859164983034134\n",
      "Epoch:37 Iteration:400 Loss:0.013937989249825478\n",
      "Epoch:37 Iteration:500 Loss:0.02263660542666912\n",
      "Epoch:37 Iteration:600 Loss:0.016407560557127\n",
      "Epoch:37 Iteration:700 Loss:0.030080750584602356\n",
      "Epoch:37 Iteration:800 Loss:0.04496246948838234\n",
      "Epoch:37 Iteration:900 Loss:0.016251737251877785\n",
      "Epoch:37 Iteration:1000 Loss:0.02161743864417076\n",
      "Epoch:37 Iteration:1100 Loss:0.01691747084259987\n",
      "Epoch:37 Iteration:1200 Loss:0.023294489830732346\n",
      "Epoch:37 Iteration:1300 Loss:0.03238294646143913\n",
      "Epoch:37 Iteration:1400 Loss:0.03600647300481796\n",
      "Epoch:37 Iteration:1500 Loss:0.021128397434949875\n",
      "Epoch:37 Iteration:1600 Loss:0.013607765547931194\n",
      "Epoch:37 Iteration:1700 Loss:0.020320270210504532\n",
      "Epoch:37 Iteration:1800 Loss:0.021966909989714622\n",
      "Epoch:37 Iteration:1900 Loss:0.011870251968502998\n",
      "Epoch:37 Iteration:2000 Loss:0.03267855569720268\n",
      "Epoch:37 Iteration:2100 Loss:0.019778400659561157\n",
      "Epoch:37 Iteration:2200 Loss:0.043294019997119904\n",
      "Epoch:37 Iteration:2300 Loss:0.02068294584751129\n",
      "Epoch:37 Iteration:2400 Loss:0.01961246319115162\n",
      "Epoch:37 Test Loss:0.07372920215129852, Train R^2:0.9756879806518555, Test R^2:0.933008074760437\n",
      "Epoch:38 Iteration:100 Loss:0.03405173867940903\n",
      "Epoch:38 Iteration:200 Loss:0.0246591717004776\n",
      "Epoch:38 Iteration:300 Loss:0.02085086889564991\n",
      "Epoch:38 Iteration:400 Loss:0.014223889447748661\n",
      "Epoch:38 Iteration:500 Loss:0.037152037024497986\n",
      "Epoch:38 Iteration:600 Loss:0.013761276379227638\n",
      "Epoch:38 Iteration:700 Loss:0.02849670685827732\n",
      "Epoch:38 Iteration:800 Loss:0.022068466991186142\n",
      "Epoch:38 Iteration:900 Loss:0.02481994591653347\n",
      "Epoch:38 Iteration:1000 Loss:0.01433095708489418\n",
      "Epoch:38 Iteration:1100 Loss:0.03673187643289566\n",
      "Epoch:38 Iteration:1200 Loss:0.01282764133065939\n",
      "Epoch:38 Iteration:1300 Loss:0.035738423466682434\n",
      "Epoch:38 Iteration:1400 Loss:0.02025945857167244\n",
      "Epoch:38 Iteration:1500 Loss:0.03238050639629364\n",
      "Epoch:38 Iteration:1600 Loss:0.027547713369131088\n",
      "Epoch:38 Iteration:1700 Loss:0.02639824151992798\n",
      "Epoch:38 Iteration:1800 Loss:0.04968319833278656\n",
      "Epoch:38 Iteration:1900 Loss:0.018428189679980278\n",
      "Epoch:38 Iteration:2000 Loss:0.017983414232730865\n",
      "Epoch:38 Iteration:2100 Loss:0.023682231083512306\n",
      "Epoch:38 Iteration:2200 Loss:0.027443043887615204\n",
      "Epoch:38 Iteration:2300 Loss:0.0237591490149498\n",
      "Epoch:38 Iteration:2400 Loss:0.012262323871254921\n",
      "Epoch:38 Test Loss:0.07478165626525879, Train R^2:0.9762963652610779, Test R^2:0.9320361614227295\n",
      "Epoch:39 Iteration:100 Loss:0.028806958347558975\n",
      "Epoch:39 Iteration:200 Loss:0.04263107478618622\n",
      "Epoch:39 Iteration:300 Loss:0.01660660095512867\n",
      "Epoch:39 Iteration:400 Loss:0.02053677663207054\n",
      "Epoch:39 Iteration:500 Loss:0.01906510815024376\n",
      "Epoch:39 Iteration:600 Loss:0.034191716462373734\n",
      "Epoch:39 Iteration:700 Loss:0.03351326286792755\n",
      "Epoch:39 Iteration:800 Loss:0.01314341090619564\n",
      "Epoch:39 Iteration:900 Loss:0.022162392735481262\n",
      "Epoch:39 Iteration:1000 Loss:0.014958638697862625\n",
      "Epoch:39 Iteration:1100 Loss:0.03197060525417328\n",
      "Epoch:39 Iteration:1200 Loss:0.022455479949712753\n",
      "Epoch:39 Iteration:1300 Loss:0.02028203383088112\n",
      "Epoch:39 Iteration:1400 Loss:0.03104517236351967\n",
      "Epoch:39 Iteration:1500 Loss:0.021690774708986282\n",
      "Epoch:39 Iteration:1600 Loss:0.017435219138860703\n",
      "Epoch:39 Iteration:1700 Loss:0.029854748398065567\n",
      "Epoch:39 Iteration:1800 Loss:0.02507651410996914\n",
      "Epoch:39 Iteration:1900 Loss:0.045423202216625214\n",
      "Epoch:39 Iteration:2000 Loss:0.015958990901708603\n",
      "Epoch:39 Iteration:2100 Loss:0.029449516907334328\n",
      "Epoch:39 Iteration:2200 Loss:0.03567397594451904\n",
      "Epoch:39 Iteration:2300 Loss:0.026298904791474342\n",
      "Epoch:39 Iteration:2400 Loss:0.02670496515929699\n",
      "Epoch:39 Test Loss:0.07555324584245682, Train R^2:0.9767776131629944, Test R^2:0.9313376545906067\n",
      "Epoch:40 Iteration:100 Loss:0.024983655661344528\n",
      "Epoch:40 Iteration:200 Loss:0.014367330819368362\n",
      "Epoch:40 Iteration:300 Loss:0.017269033938646317\n",
      "Epoch:40 Iteration:400 Loss:0.02510925754904747\n",
      "Epoch:40 Iteration:500 Loss:0.0217098668217659\n",
      "Epoch:40 Iteration:600 Loss:0.028698747977614403\n",
      "Epoch:40 Iteration:700 Loss:0.025767598301172256\n",
      "Epoch:40 Iteration:800 Loss:0.017388420179486275\n",
      "Epoch:40 Iteration:900 Loss:0.020988747477531433\n",
      "Epoch:40 Iteration:1000 Loss:0.023465890437364578\n",
      "Epoch:40 Iteration:1100 Loss:0.015901265665888786\n",
      "Epoch:40 Iteration:1200 Loss:0.03333815187215805\n",
      "Epoch:40 Iteration:1300 Loss:0.029178578406572342\n",
      "Epoch:40 Iteration:1400 Loss:0.035682544112205505\n",
      "Epoch:40 Iteration:1500 Loss:0.019147807732224464\n",
      "Epoch:40 Iteration:1600 Loss:0.02942008525133133\n",
      "Epoch:40 Iteration:1700 Loss:0.017168568447232246\n",
      "Epoch:40 Iteration:1800 Loss:0.012396221980452538\n",
      "Epoch:40 Iteration:1900 Loss:0.018654540181159973\n",
      "Epoch:40 Iteration:2000 Loss:0.02553611248731613\n",
      "Epoch:40 Iteration:2100 Loss:0.01988951303064823\n",
      "Epoch:40 Iteration:2200 Loss:0.015705782920122147\n",
      "Epoch:40 Iteration:2300 Loss:0.024261144921183586\n",
      "Epoch:40 Iteration:2400 Loss:0.023648886010050774\n",
      "Epoch:40 Test Loss:0.07543911039829254, Train R^2:0.9769741892814636, Test R^2:0.931441068649292\n",
      "Epoch:41 Iteration:100 Loss:0.021478068083524704\n",
      "Epoch:41 Iteration:200 Loss:0.02497760020196438\n",
      "Epoch:41 Iteration:300 Loss:0.026968594640493393\n",
      "Epoch:41 Iteration:400 Loss:0.020030945539474487\n",
      "Epoch:41 Iteration:500 Loss:0.017534634098410606\n",
      "Epoch:41 Iteration:600 Loss:0.017402295023202896\n",
      "Epoch:41 Iteration:700 Loss:0.026903735473752022\n",
      "Epoch:41 Iteration:800 Loss:0.01283018384128809\n",
      "Epoch:41 Iteration:900 Loss:0.014156393706798553\n",
      "Epoch:41 Iteration:1000 Loss:0.03212320804595947\n",
      "Epoch:41 Iteration:1100 Loss:0.02234639599919319\n",
      "Epoch:41 Iteration:1200 Loss:0.02653535269200802\n",
      "Epoch:41 Iteration:1300 Loss:0.019936565309762955\n",
      "Epoch:41 Iteration:1400 Loss:0.014138411730527878\n",
      "Epoch:41 Iteration:1500 Loss:0.01755441725254059\n",
      "Epoch:41 Iteration:1600 Loss:0.01773548498749733\n",
      "Epoch:41 Iteration:1700 Loss:0.043392546474933624\n",
      "Epoch:41 Iteration:1800 Loss:0.027348574250936508\n",
      "Epoch:41 Iteration:1900 Loss:0.029230104759335518\n",
      "Epoch:41 Iteration:2000 Loss:0.02526251971721649\n",
      "Epoch:41 Iteration:2100 Loss:0.023397380486130714\n",
      "Epoch:41 Iteration:2200 Loss:0.016453631222248077\n",
      "Epoch:41 Iteration:2300 Loss:0.014505507424473763\n",
      "Epoch:41 Iteration:2400 Loss:0.030562635511159897\n",
      "Epoch:41 Test Loss:0.07602466642856598, Train R^2:0.9775898456573486, Test R^2:0.9309003353118896\n",
      "Epoch:42 Iteration:100 Loss:0.022094331681728363\n",
      "Epoch:42 Iteration:200 Loss:0.015301033854484558\n",
      "Epoch:42 Iteration:300 Loss:0.01997261308133602\n",
      "Epoch:42 Iteration:400 Loss:0.0274299718439579\n",
      "Epoch:42 Iteration:500 Loss:0.01631864905357361\n",
      "Epoch:42 Iteration:600 Loss:0.018766280263662338\n",
      "Epoch:42 Iteration:700 Loss:0.019040051847696304\n",
      "Epoch:42 Iteration:800 Loss:0.025939567014575005\n",
      "Epoch:42 Iteration:900 Loss:0.026926156133413315\n",
      "Epoch:42 Iteration:1000 Loss:0.024996234104037285\n",
      "Epoch:42 Iteration:1100 Loss:0.0207129567861557\n",
      "Epoch:42 Iteration:1200 Loss:0.027140595018863678\n",
      "Epoch:42 Iteration:1300 Loss:0.024820800870656967\n",
      "Epoch:42 Iteration:1400 Loss:0.023898426443338394\n",
      "Epoch:42 Iteration:1500 Loss:0.018767764791846275\n",
      "Epoch:42 Iteration:1600 Loss:0.026149842888116837\n",
      "Epoch:42 Iteration:1700 Loss:0.02874351106584072\n",
      "Epoch:42 Iteration:1800 Loss:0.02438417077064514\n",
      "Epoch:42 Iteration:1900 Loss:0.04371903836727142\n",
      "Epoch:42 Iteration:2000 Loss:0.025579486042261124\n",
      "Epoch:42 Iteration:2100 Loss:0.01687634363770485\n",
      "Epoch:42 Iteration:2200 Loss:0.023129798471927643\n",
      "Epoch:42 Iteration:2300 Loss:0.027215266600251198\n",
      "Epoch:42 Iteration:2400 Loss:0.03151217848062515\n",
      "Epoch:42 Test Loss:0.07540899515151978, Train R^2:0.9776636362075806, Test R^2:0.9315019845962524\n",
      "Epoch:43 Iteration:100 Loss:0.019344810396432877\n",
      "Epoch:43 Iteration:200 Loss:0.021683871746063232\n",
      "Epoch:43 Iteration:300 Loss:0.027028560638427734\n",
      "Epoch:43 Iteration:400 Loss:0.04210248216986656\n",
      "Epoch:43 Iteration:500 Loss:0.025580521672964096\n",
      "Epoch:43 Iteration:600 Loss:0.013414159417152405\n",
      "Epoch:43 Iteration:700 Loss:0.03324494883418083\n",
      "Epoch:43 Iteration:800 Loss:0.01428051758557558\n",
      "Epoch:43 Iteration:900 Loss:0.012530002743005753\n",
      "Epoch:43 Iteration:1000 Loss:0.03377578780055046\n",
      "Epoch:43 Iteration:1100 Loss:0.01939474046230316\n",
      "Epoch:43 Iteration:1200 Loss:0.02032068558037281\n",
      "Epoch:43 Iteration:1300 Loss:0.0136532848700881\n",
      "Epoch:43 Iteration:1400 Loss:0.021618613973259926\n",
      "Epoch:43 Iteration:1500 Loss:0.018477346748113632\n",
      "Epoch:43 Iteration:1600 Loss:0.020385734736919403\n",
      "Epoch:43 Iteration:1700 Loss:0.014237122610211372\n",
      "Epoch:43 Iteration:1800 Loss:0.028421254828572273\n",
      "Epoch:43 Iteration:1900 Loss:0.018340520560741425\n",
      "Epoch:43 Iteration:2000 Loss:0.019919632002711296\n",
      "Epoch:43 Iteration:2100 Loss:0.01727459579706192\n",
      "Epoch:43 Iteration:2200 Loss:0.0156997162848711\n",
      "Epoch:43 Iteration:2300 Loss:0.027789201587438583\n",
      "Epoch:43 Iteration:2400 Loss:0.016169473528862\n",
      "Epoch:43 Test Loss:0.07683867961168289, Train R^2:0.9784189462661743, Test R^2:0.9301625490188599\n",
      "Epoch:44 Iteration:100 Loss:0.02229713834822178\n",
      "Epoch:44 Iteration:200 Loss:0.01448429562151432\n",
      "Epoch:44 Iteration:300 Loss:0.01652202382683754\n",
      "Epoch:44 Iteration:400 Loss:0.018404707312583923\n",
      "Epoch:44 Iteration:500 Loss:0.019639194011688232\n",
      "Epoch:44 Iteration:600 Loss:0.029965244233608246\n",
      "Epoch:44 Iteration:700 Loss:0.02142493799328804\n",
      "Epoch:44 Iteration:800 Loss:0.014100128784775734\n",
      "Epoch:44 Iteration:900 Loss:0.021487552672624588\n",
      "Epoch:44 Iteration:1000 Loss:0.019517477601766586\n",
      "Epoch:44 Iteration:1100 Loss:0.03941328823566437\n",
      "Epoch:44 Iteration:1200 Loss:0.029505060985684395\n",
      "Epoch:44 Iteration:1300 Loss:0.02172720432281494\n",
      "Epoch:44 Iteration:1400 Loss:0.03324170410633087\n",
      "Epoch:44 Iteration:1500 Loss:0.04263084754347801\n",
      "Epoch:44 Iteration:1600 Loss:0.020952248945832253\n",
      "Epoch:44 Iteration:1700 Loss:0.018306076526641846\n",
      "Epoch:44 Iteration:1800 Loss:0.01968592032790184\n",
      "Epoch:44 Iteration:1900 Loss:0.019218429923057556\n",
      "Epoch:44 Iteration:2000 Loss:0.01310473121702671\n",
      "Epoch:44 Iteration:2100 Loss:0.017665019258856773\n",
      "Epoch:44 Iteration:2200 Loss:0.009345876052975655\n",
      "Epoch:44 Iteration:2300 Loss:0.013380719348788261\n",
      "Epoch:44 Iteration:2400 Loss:0.011471564881503582\n",
      "Epoch:44 Test Loss:0.07602029293775558, Train R^2:0.9784482717514038, Test R^2:0.9309121370315552\n",
      "Epoch:45 Iteration:100 Loss:0.022657286375761032\n",
      "Epoch:45 Iteration:200 Loss:0.048382148146629333\n",
      "Epoch:45 Iteration:300 Loss:0.01508919708430767\n",
      "Epoch:45 Iteration:400 Loss:0.016387872397899628\n",
      "Epoch:45 Iteration:500 Loss:0.028552085161209106\n",
      "Epoch:45 Iteration:600 Loss:0.01758323237299919\n",
      "Epoch:45 Iteration:700 Loss:0.039171308279037476\n",
      "Epoch:45 Iteration:800 Loss:0.018095772713422775\n",
      "Epoch:45 Iteration:900 Loss:0.03267161175608635\n",
      "Epoch:45 Iteration:1000 Loss:0.018466513603925705\n",
      "Epoch:45 Iteration:1100 Loss:0.024523906409740448\n",
      "Epoch:45 Iteration:1200 Loss:0.01360989362001419\n",
      "Epoch:45 Iteration:1300 Loss:0.023091012611985207\n",
      "Epoch:45 Iteration:1400 Loss:0.02405478060245514\n",
      "Epoch:45 Iteration:1500 Loss:0.014888709411025047\n",
      "Epoch:45 Iteration:1600 Loss:0.015548545867204666\n",
      "Epoch:45 Iteration:1700 Loss:0.024994730949401855\n",
      "Epoch:45 Iteration:1800 Loss:0.007459363434463739\n",
      "Epoch:45 Iteration:1900 Loss:0.024571329355239868\n",
      "Epoch:45 Iteration:2000 Loss:0.020264064893126488\n",
      "Epoch:45 Iteration:2100 Loss:0.020854240283370018\n",
      "Epoch:45 Iteration:2200 Loss:0.02575177699327469\n",
      "Epoch:45 Iteration:2300 Loss:0.013772772625088692\n",
      "Epoch:45 Iteration:2400 Loss:0.019120551645755768\n",
      "Epoch:45 Test Loss:0.07576117664575577, Train R^2:0.9778144955635071, Test R^2:0.9312276840209961\n",
      "Epoch:46 Iteration:100 Loss:0.010357293300330639\n",
      "Epoch:46 Iteration:200 Loss:0.01362205483019352\n",
      "Epoch:46 Iteration:300 Loss:0.02000400796532631\n",
      "Epoch:46 Iteration:400 Loss:0.02871033176779747\n",
      "Epoch:46 Iteration:500 Loss:0.019599687308073044\n",
      "Epoch:46 Iteration:600 Loss:0.016686350107192993\n",
      "Epoch:46 Iteration:700 Loss:0.05390757694840431\n",
      "Epoch:46 Iteration:800 Loss:0.011917625553905964\n",
      "Epoch:46 Iteration:900 Loss:0.026814691722393036\n",
      "Epoch:46 Iteration:1000 Loss:0.01944502629339695\n",
      "Epoch:46 Iteration:1100 Loss:0.013026592321693897\n",
      "Epoch:46 Iteration:1200 Loss:0.02357793226838112\n",
      "Epoch:46 Iteration:1300 Loss:0.020681075751781464\n",
      "Epoch:46 Iteration:1400 Loss:0.019150208681821823\n",
      "Epoch:46 Iteration:1500 Loss:0.027890551835298538\n",
      "Epoch:46 Iteration:1600 Loss:0.038698017597198486\n",
      "Epoch:46 Iteration:1700 Loss:0.01768762245774269\n",
      "Epoch:46 Iteration:1800 Loss:0.02314641699194908\n",
      "Epoch:46 Iteration:1900 Loss:0.027164801955223083\n",
      "Epoch:46 Iteration:2000 Loss:0.025597769767045975\n",
      "Epoch:46 Iteration:2100 Loss:0.023033224046230316\n",
      "Epoch:46 Iteration:2200 Loss:0.030667057260870934\n",
      "Epoch:46 Iteration:2300 Loss:0.011608544737100601\n",
      "Epoch:46 Iteration:2400 Loss:0.0347011536359787\n",
      "Epoch:46 Test Loss:0.07575178146362305, Train R^2:0.9792238473892212, Test R^2:0.931148886680603\n",
      "Epoch:47 Iteration:100 Loss:0.025808431208133698\n",
      "Epoch:47 Iteration:200 Loss:0.03285534307360649\n",
      "Epoch:47 Iteration:300 Loss:0.024740124121308327\n",
      "Epoch:47 Iteration:400 Loss:0.019892632961273193\n",
      "Epoch:47 Iteration:500 Loss:0.020850613713264465\n",
      "Epoch:47 Iteration:600 Loss:0.01226224284619093\n",
      "Epoch:47 Iteration:700 Loss:0.015677452087402344\n",
      "Epoch:47 Iteration:800 Loss:0.013149052858352661\n",
      "Epoch:47 Iteration:900 Loss:0.015703022480010986\n",
      "Epoch:47 Iteration:1000 Loss:0.014156808145344257\n",
      "Epoch:47 Iteration:1100 Loss:0.013010494410991669\n",
      "Epoch:47 Iteration:1200 Loss:0.032180316746234894\n",
      "Epoch:47 Iteration:1300 Loss:0.010413644835352898\n",
      "Epoch:47 Iteration:1400 Loss:0.015532180666923523\n",
      "Epoch:47 Iteration:1500 Loss:0.00999772734940052\n",
      "Epoch:47 Iteration:1600 Loss:0.016038045287132263\n",
      "Epoch:47 Iteration:1700 Loss:0.017129940912127495\n",
      "Epoch:47 Iteration:1800 Loss:0.013289574533700943\n",
      "Epoch:47 Iteration:1900 Loss:0.02230195514857769\n",
      "Epoch:47 Iteration:2000 Loss:0.030179858207702637\n",
      "Epoch:47 Iteration:2100 Loss:0.021961435675621033\n",
      "Epoch:47 Iteration:2200 Loss:0.016786735504865646\n",
      "Epoch:47 Iteration:2300 Loss:0.013085639104247093\n",
      "Epoch:47 Iteration:2400 Loss:0.022328736260533333\n",
      "Epoch:47 Test Loss:0.07828339189291, Train R^2:0.9793996810913086, Test R^2:0.9288756847381592\n",
      "Epoch:48 Iteration:100 Loss:0.012975666671991348\n",
      "Epoch:48 Iteration:200 Loss:0.018945053219795227\n",
      "Epoch:48 Iteration:300 Loss:0.023807264864444733\n",
      "Epoch:48 Iteration:400 Loss:0.016563229262828827\n",
      "Epoch:48 Iteration:500 Loss:0.014694673009216785\n",
      "Epoch:48 Iteration:600 Loss:0.0458785742521286\n",
      "Epoch:48 Iteration:700 Loss:0.02671891078352928\n",
      "Epoch:48 Iteration:800 Loss:0.02503855898976326\n",
      "Epoch:48 Iteration:900 Loss:0.02213822305202484\n",
      "Epoch:48 Iteration:1000 Loss:0.03229926526546478\n",
      "Epoch:48 Iteration:1100 Loss:0.014690397307276726\n",
      "Epoch:48 Iteration:1200 Loss:0.015572363510727882\n",
      "Epoch:48 Iteration:1300 Loss:0.018846631050109863\n",
      "Epoch:48 Iteration:1400 Loss:0.03614507615566254\n",
      "Epoch:48 Iteration:1500 Loss:0.023921720683574677\n",
      "Epoch:48 Iteration:1600 Loss:0.014511922374367714\n",
      "Epoch:48 Iteration:1700 Loss:0.018911082297563553\n",
      "Epoch:48 Iteration:1800 Loss:0.025258474051952362\n",
      "Epoch:48 Iteration:1900 Loss:0.024203099310398102\n",
      "Epoch:48 Iteration:2000 Loss:0.021445762366056442\n",
      "Epoch:48 Iteration:2100 Loss:0.028998281806707382\n",
      "Epoch:48 Iteration:2200 Loss:0.030087940394878387\n",
      "Epoch:48 Iteration:2300 Loss:0.01710423454642296\n",
      "Epoch:48 Iteration:2400 Loss:0.02162051573395729\n",
      "Epoch:48 Test Loss:0.07866743206977844, Train R^2:0.9796535968780518, Test R^2:0.9285948872566223\n",
      "Epoch:49 Iteration:100 Loss:0.021646063774824142\n",
      "Epoch:49 Iteration:200 Loss:0.019671734422445297\n",
      "Epoch:49 Iteration:300 Loss:0.011640911921858788\n",
      "Epoch:49 Iteration:400 Loss:0.015160668641328812\n",
      "Epoch:49 Iteration:500 Loss:0.02279829978942871\n",
      "Epoch:49 Iteration:600 Loss:0.008283915929496288\n",
      "Epoch:49 Iteration:700 Loss:0.019466258585453033\n",
      "Epoch:49 Iteration:800 Loss:0.01728586107492447\n",
      "Epoch:49 Iteration:900 Loss:0.01749321259558201\n",
      "Epoch:49 Iteration:1000 Loss:0.013067694380879402\n",
      "Epoch:49 Iteration:1100 Loss:0.025027338415384293\n",
      "Epoch:49 Iteration:1200 Loss:0.012732909992337227\n",
      "Epoch:49 Iteration:1300 Loss:0.013119788840413094\n",
      "Epoch:49 Iteration:1400 Loss:0.01751810684800148\n",
      "Epoch:49 Iteration:1500 Loss:0.023873820900917053\n",
      "Epoch:49 Iteration:1600 Loss:0.023032870143651962\n",
      "Epoch:49 Iteration:1700 Loss:0.033201176673173904\n",
      "Epoch:49 Iteration:1800 Loss:0.029052970930933952\n",
      "Epoch:49 Iteration:1900 Loss:0.03734523057937622\n",
      "Epoch:49 Iteration:2000 Loss:0.0340212807059288\n",
      "Epoch:49 Iteration:2100 Loss:0.02646566927433014\n",
      "Epoch:49 Iteration:2200 Loss:0.014358832500874996\n",
      "Epoch:49 Iteration:2300 Loss:0.027137720957398415\n",
      "Epoch:49 Iteration:2400 Loss:0.013239005580544472\n",
      "Epoch:49 Test Loss:0.08154486119747162, Train R^2:0.9798217415809631, Test R^2:0.9258949160575867\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGdCAYAAAA1/PiZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAx5ElEQVR4nO3dfXRU5aHH+9+e17yQmRDzrgFBEKq8KWqM9fWSCmlrgfZ4KIdzQNvSVQ/26qVaS4+KrV0rbV21nqMUe86qxi6rWM9VONdabjGK1APYAkbFFy7BQEBIgEAyeZ3X5/4RmDKSkIROMtnh+1lrr8Xe+3n2PPNkmPnN3vuZxzLGGAEAANiAI9UNAAAA6C+CCwAAsA2CCwAAsA2CCwAAsA2CCwAAsA2CCwAAsA2CCwAAsA2CCwAAsA1XqhuQDLFYTAcPHlRWVpYsy0p1cwAAQD8YY9Ta2qri4mI5HP07lzIigsvBgwdVUlKS6mYAAICzsH//fl1wwQX9KjsigktWVpak7ifu8/lS3BoAANAfgUBAJSUl8c/x/hgRweXk5SGfz0dwAQDAZgZym8eAb87dtGmTbrnlFhUXF8uyLK1du/a0B+9peeSRR3o95kMPPXRa+cmTJw+0aQAAYIQbcHBpb2/X9OnTtWrVqh73Hzp0KGF56qmnZFmWvva1r53xuJdeemlCvbfeemugTQMAACPcgC8VVVRUqKKiotf9hYWFCevr1q3TTTfdpPHjx5+5IS7XaXUBAABONai/49LY2Kg//OEP+uY3v9ln2d27d6u4uFjjx4/XokWLVF9f32vZYDCoQCCQsAAAgJFvUIPLM888o6ysLH31q189Y7nS0lJVVVVp/fr1Wr16terq6nTdddeptbW1x/KVlZXy+/3xhaHQAACcGyxjjDnrypall19+WfPmzetx/+TJk/WFL3xBjz/++ICO29zcrLFjx+rRRx/t8WxNMBhUMBiMr58cTtXS0sKoIgAAbCIQCMjv9w/o83vQhkP/+c9/1q5du/TCCy8MuG52drYuvvhi1dbW9rjf6/XK6/X+vU0EAAA2M2iXin7zm99o5syZmj59+oDrtrW1ac+ePSoqKhqElgEAALsacHBpa2tTTU2NampqJEl1dXWqqalJuJk2EAjoxRdf1Le+9a0ejzFr1iw98cQT8fV77rlHb775pvbu3avNmzdr/vz5cjqdWrhw4UCbBwAARrABXyratm2bbrrppvj68uXLJUlLlixRVVWVJGnNmjUyxvQaPPbs2aOjR4/G1w8cOKCFCxeqqalJeXl5uvbaa7V161bl5eUNtHkAAGAE+7tuzh0uzubmHgAAkFpn8/k9qMOhAQAAkmlETLI4WCLRmH7yh48kST+omKw0tzPFLQIA4NzGGZcziBmpavNeVW3eq1A0lurmAABwziO4AAAA2yC4AAAA2yC4AAAA2yC4AAAA2yC4AAAA2yC49JP9f6YPAAD7I7icgWWlugUAAOBUBBcAAGAbBBcAAGAbBBcAAGAbBBcAAGAbBBcAAGAbBJf+Yjg0AAApR3A5A0ZDAwAwvBBcAACAbRBcAACAbRBcAACAbRBcAACAbRBc+skwrAgAgJQjuJyBxSyLAAAMKwQXAABgGwQXAABgGwQXAABgGwQXAABgGwQXAABgGwSXfjKMhgYAIOUILmfAYGgAAIYXggsAALANggsAALANggsAALANggsAALANggsAALANgks/MRoaAIDUI7icAZNDAwAwvBBcAACAbQw4uGzatEm33HKLiouLZVmW1q5dm7D/tttuk2VZCcucOXP6PO6qVat04YUXKi0tTaWlpfrLX/4y0KYBAIARbsDBpb29XdOnT9eqVat6LTNnzhwdOnQovjz//PNnPOYLL7yg5cuXa+XKldqxY4emT5+u2bNn6/DhwwNtHgAAGMFcA61QUVGhioqKM5bxer0qLCzs9zEfffRRLV26VLfffrsk6cknn9Qf/vAHPfXUU/rBD34w0CYCAIARalDucdm4caPy8/M1adIk3XHHHWpqauq1bCgU0vbt21VeXv63RjkcKi8v15YtW3qsEwwGFQgEEhYAADDyJT24zJkzR7/97W9VXV2tn/3sZ3rzzTdVUVGhaDTaY/mjR48qGo2qoKAgYXtBQYEaGhp6rFNZWSm/3x9fSkpKkv00TmOYHhoAgJQb8KWivnz961+P/3vq1KmaNm2aLrroIm3cuFGzZs1KymOsWLFCy5cvj68HAoFBCS8W46EBABhWBn049Pjx45Wbm6va2toe9+fm5srpdKqxsTFhe2NjY6/3yXi9Xvl8voQFAACMfIMeXA4cOKCmpiYVFRX1uN/j8WjmzJmqrq6Ob4vFYqqurlZZWdlgNw8AANjIgINLW1ubampqVFNTI0mqq6tTTU2N6uvr1dbWpnvvvVdbt27V3r17VV1drblz52rChAmaPXt2/BizZs3SE088EV9fvny5/uu//kvPPPOMPvroI91xxx1qb2+PjzICAACQzuIel23btummm26Kr5+812TJkiVavXq13nvvPT3zzDNqbm5WcXGxbr75Zj388MPyer3xOnv27NHRo0fj6wsWLNCRI0f04IMPqqGhQTNmzND69etPu2EXAACc2ywzAobLBAIB+f1+tbS0JP1+lwt/8AdJ0vb7y3XeKG8fpQEAQH+dzec3cxX1k+3THQAAIwDBBQAA2AbBBQAA2AbBBQAA2AbBBQAA2AbBBQAA2AbBBQAA2AbBpZ/s/2s3AADYH8GlD0wQDQDA8EFwAQAAtkFwAQAAtkFwAQAAtkFwAQAAtkFw6SfDNIsAAKQcwaUPDCoCAGD4ILgAAADbILgAAADbILgAAADbILgAAADbILgAAADbILj0F6OhAQBIOYJLHyxmWQQAYNgguAAAANsguAAAANsguAAAANsguAAAANsguAAAANsguPQTo6EBAEg9gksfGAwNAMDwQXABAAC2QXABAAC2QXABAAC2QXABAAC2QXABAAC2QXDpJ8N4aAAAUo7g0gcmhwYAYPgguAAAANsguAAAANsguAAAANsYcHDZtGmTbrnlFhUXF8uyLK1duza+LxwO67777tPUqVOVmZmp4uJiLV68WAcPHjzjMR966CFZlpWwTJ48ecBPBgAAjGwDDi7t7e2aPn26Vq1addq+jo4O7dixQw888IB27Nihl156Sbt27dJXvvKVPo976aWX6tChQ/HlrbfeGmjTAADACOcaaIWKigpVVFT0uM/v92vDhg0J25544gldddVVqq+v15gxY3pviMulwsLCgTZnyBjmhwYAIOUG/R6XlpYWWZal7OzsM5bbvXu3iouLNX78eC1atEj19fW9lg0GgwoEAgnLYLGYHxoAgGFjUINLV1eX7rvvPi1cuFA+n6/XcqWlpaqqqtL69eu1evVq1dXV6brrrlNra2uP5SsrK+X3++NLSUnJYD0FAAAwjAxacAmHw/rHf/xHGWO0evXqM5atqKjQrbfeqmnTpmn27Nl69dVX1dzcrN///vc9ll+xYoVaWlriy/79+wfjKQAAgGFmwPe49MfJ0LJv3z69/vrrZzzb0pPs7GxdfPHFqq2t7XG/1+uV1+tNRlMBAICNJP2My8nQsnv3br322ms677zzBnyMtrY27dmzR0VFRcluHgAAsLEBB5e2tjbV1NSopqZGklRXV6eamhrV19crHA7rH/7hH7Rt2zb97ne/UzQaVUNDgxoaGhQKheLHmDVrlp544on4+j333KM333xTe/fu1ebNmzV//nw5nU4tXLjw73+GAABgxBjwpaJt27bppptuiq8vX75ckrRkyRI99NBD+p//+R9J0owZMxLqvfHGG7rxxhslSXv27NHRo0fj+w4cOKCFCxeqqalJeXl5uvbaa7V161bl5eUNtHmDhtmhAQBIvQEHlxtvvFHmDJ/iZ9p30t69exPW16xZM9BmDB1GQwMAMGwwVxEAALANggsAALANggsAALANggsAALANgks/MagIAIDUI7j0gUFFAAAMHwQXAABgGwQXAABgGwQXAABgGwQXAABgGwQXAABgGwSXfurPHEwAAGBwEVz6YDEeGgCAYYPgAgAAbIPgAgAAbIPgAgAAbIPgAgAAbIPgAgAAbIPg0k+MhgYAIPUILn2wmB8aAIBhg+ACAABsg+ACAABsg+ACAABsg+ACAABsg+ACAABsg+ACAABsg+DSB2aHBgBg+CC4AAAA2yC4AAAA2yC4AAAA2yC4AAAA2yC4AAAA2yC49BOzQwMAkHoElz4wGhoAgOGD4AIAAGyD4AIAAGyD4AIAAGyD4AIAAGxjwMFl06ZNuuWWW1RcXCzLsrR27dqE/cYYPfjggyoqKlJ6errKy8u1e/fuPo+7atUqXXjhhUpLS1Npaan+8pe/DLRpg8qIYUUAAKTagINLe3u7pk+frlWrVvW4/+c//7n+4z/+Q08++aTefvttZWZmavbs2erq6ur1mC+88IKWL1+ulStXaseOHZo+fbpmz56tw4cPD7R5SWcxyyIAAMPGgINLRUWFfvKTn2j+/Pmn7TPG6LHHHtP999+vuXPnatq0afrtb3+rgwcPnnZm5lSPPvqoli5dqttvv12XXHKJnnzySWVkZOipp54aaPMAAMAIltR7XOrq6tTQ0KDy8vL4Nr/fr9LSUm3ZsqXHOqFQSNu3b0+o43A4VF5e3mudYDCoQCCQsAAAgJEvqcGloaFBklRQUJCwvaCgIL7vs44ePapoNDqgOpWVlfL7/fGlpKQkCa0HAADDnS1HFa1YsUItLS3xZf/+/aluEgAAGAJJDS6FhYWSpMbGxoTtjY2N8X2flZubK6fTOaA6Xq9XPp8vYQEAACNfUoPLuHHjVFhYqOrq6vi2QCCgt99+W2VlZT3W8Xg8mjlzZkKdWCym6urqXuukApMsAgCQeq6BVmhra1NtbW18va6uTjU1NcrJydGYMWN099136yc/+YkmTpyocePG6YEHHlBxcbHmzZsXrzNr1izNnz9fd955pyRp+fLlWrJkia644gpdddVVeuyxx9Te3q7bb7/973+GfycGQwMAMHwMOLhs27ZNN910U3x9+fLlkqQlS5aoqqpK3//+99Xe3q5vf/vbam5u1rXXXqv169crLS0tXmfPnj06evRofH3BggU6cuSIHnzwQTU0NGjGjBlav379aTfsAgCAc5tljP0vggQCAfn9frW0tCT9fpepK/9ftQYj2njPjbowNzOpxwYA4Fx2Np/fthxVBAAAzk0EFwAAYBsEFwAAYBsEl36y/Y1AAACMAASXvjAeGgCAYYPgAgAAbIPgAgAAbIPgAgAAbIPgAgAAbIPgAgAAbIPg0k8jYGYEAABsj+DSB0ZDAwAwfBBcAACAbRBcAACAbRBcAACAbRBcAACAbRBcAACAbRBc+onB0AAApB7BpQ+WxYBoAACGC4ILAACwDYILAACwDYILAACwDYILAACwDYILAACwDYJLPzE5NAAAqUdw6QOjoQEAGD4ILgAAwDYILgAAwDYILgAAwDYILgAAwDYILv3GsCIAAFKN4NIHBhUBADB8EFwAAIBtEFwAAIBtEFwAAIBtEFwAAIBtEFwAAIBtEFz6iUkWAQBIvaQHlwsvvFCWZZ22LFu2rMfyVVVVp5VNS0tLdrPOmsUsiwAADBuuZB/wr3/9q6LRaHx9586d+sIXvqBbb7211zo+n0+7du2KrxMWAABAT5IeXPLy8hLWf/rTn+qiiy7SDTfc0Gsdy7JUWFiY7KYAAIARZlDvcQmFQnr22Wf1jW9844xnUdra2jR27FiVlJRo7ty5+uCDDwazWQAAwKYGNbisXbtWzc3Nuu2223otM2nSJD311FNat26dnn32WcViMV1zzTU6cOBAr3WCwaACgUDCAgAARr5BDS6/+c1vVFFRoeLi4l7LlJWVafHixZoxY4ZuuOEGvfTSS8rLy9Ovf/3rXutUVlbK7/fHl5KSksFoPgAAGGYGLbjs27dPr732mr71rW8NqJ7b7dZll12m2traXsusWLFCLS0t8WX//v1/b3P7xGhoAABSb9CCy9NPP638/Hx96UtfGlC9aDSq999/X0VFRb2W8Xq98vl8CctgYXwTAADDx6AEl1gspqefflpLliyRy5U4cGnx4sVasWJFfP3HP/6x/vSnP+mTTz7Rjh079M///M/at2/fgM/UAACAkS/pw6El6bXXXlN9fb2+8Y1vnLavvr5eDsff8tLx48e1dOlSNTQ0aPTo0Zo5c6Y2b96sSy65ZDCaBgAAbMwyxv4/Zh8IBOT3+9XS0pL0y0YzH96gpvaQ/vR/Xa+LC7KSemwAAM5lZ/P5zVxFAADANgguAADANggu/WT/C2oAANgfwaUPzPcIAMDwQXABAAC2QXABAAC2QXABAAC2QXABAAC2QXABAAC2QXDpJ8P80AAApBzBpU+MhwYAYLgguAAAANsguAAAANsguAAAANsguAAAANsguAAAANsguPQTs0MDAJB6BJc+MDs0AADDB8EFAADYBsEFAADYBsEFAADYBsEFAADYBsGlnxhVBABA6hFc+sCgIgAAhg+CCwAAsA2CCwAAsA2CCwAAsA2CCwAAsA2CCwAAsA2CSz8ZMR4aAIBUI7j0gUkWAQAYPgguAADANgguAADANgguAADANgguAADANgguAADANggu/cTs0AAApB7BpQ8W80MDADBsEFwAAIBtJD24PPTQQ7IsK2GZPHnyGeu8+OKLmjx5stLS0jR16lS9+uqryW4WAAAYAQbljMull16qQ4cOxZe33nqr17KbN2/WwoUL9c1vflPvvPOO5s2bp3nz5mnnzp2D0TQAAGBjgxJcXC6XCgsL40tubm6vZf/93/9dc+bM0b333qvPfe5zevjhh3X55ZfriSeeGIymAQAAGxuU4LJ7924VFxdr/PjxWrRokerr63stu2XLFpWXlydsmz17trZs2dJrnWAwqEAgkLAAAICRL+nBpbS0VFVVVVq/fr1Wr16turo6XXfddWptbe2xfENDgwoKChK2FRQUqKGhodfHqKyslN/vjy8lJSVJfQ4AAGB4Snpwqaio0K233qpp06Zp9uzZevXVV9Xc3Kzf//73SXuMFStWqKWlJb7s378/acf+LGaHBgBg+HAN9gNkZ2fr4osvVm1tbY/7CwsL1djYmLCtsbFRhYWFvR7T6/XK6/UmtZ0AAGD4G/TfcWlra9OePXtUVFTU4/6ysjJVV1cnbNuwYYPKysoGu2kAAMBmkh5c7rnnHr355pvau3evNm/erPnz58vpdGrhwoWSpMWLF2vFihXx8nfddZfWr1+vX/ziF/r444/10EMPadu2bbrzzjuT3TQAAGBzSb9UdODAAS1cuFBNTU3Ky8vTtddeq61btyovL0+SVF9fL4fjb3npmmuu0XPPPaf7779fP/zhDzVx4kStXbtWU6ZMSXbTAACAzSU9uKxZs+aM+zdu3HjatltvvVW33nprspsCAABGGOYq6idmhwYAIPUILn1gNDQAAMMHwQUAANgGwQUAANgGwQUAANgGwQUAANgGwQUAANgGwaWfjBgPDQBAqhFc+mAxPTQAAMMGwQUAANgGwQUAANgGwQUAANgGwQUAANgGwaWfmGQRAIDUI7gAAADbILgAAADbILgAAADbILgAAADbILgAAADbILgAAADbILj0E6OhAQBIPYJLH5hjEQCA4YPgAgAAbIPgAgAAbIPgAgAAbIPgAgAAbIPgAgAAbIPg0k+G6aEBAEg5gksfGA4NAMDwQXABAAC2QXABAAC2QXABAAC2QXABAAC2QXABAAC2QXDpJwZDAwCQegSXPlhiPDQAAMMFwQUAANgGwQUAANhG0oNLZWWlrrzySmVlZSk/P1/z5s3Trl27zlinqqpKlmUlLGlpacluGgAAsLmkB5c333xTy5Yt09atW7VhwwaFw2HdfPPNam9vP2M9n8+nQ4cOxZd9+/Ylu2kAAMDmXMk+4Pr16xPWq6qqlJ+fr+3bt+v666/vtZ5lWSosLEx2cwAAwAgy6Pe4tLS0SJJycnLOWK6trU1jx45VSUmJ5s6dqw8++KDXssFgUIFAIGEZbEwODQBA6g1qcInFYrr77rv1+c9/XlOmTOm13KRJk/TUU09p3bp1evbZZxWLxXTNNdfowIEDPZavrKyU3++PLyUlJYP1FJgdGgCAYcQyZvDOJdxxxx364x//qLfeeksXXHBBv+uFw2F97nOf08KFC/Xwww+ftj8YDCoYDMbXA4GASkpK1NLSIp/Pl5S2n3TDI29oX1OH/u87rtHMsaOTemwAAM5lgUBAfr9/QJ/fSb/H5aQ777xTr7zyijZt2jSg0CJJbrdbl112mWpra3vc7/V65fV6k9FMAABgI0m/VGSM0Z133qmXX35Zr7/+usaNGzfgY0SjUb3//vsqKipKdvMAAICNJf2My7Jly/Tcc89p3bp1ysrKUkNDgyTJ7/crPT1dkrR48WKdf/75qqyslCT9+Mc/1tVXX60JEyaoublZjzzyiPbt26dvfetbyW4eAACwsaQHl9WrV0uSbrzxxoTtTz/9tG677TZJUn19vRyOv53sOX78uJYuXaqGhgaNHj1aM2fO1ObNm3XJJZcku3l/B4YVAQCQakkPLv2513fjxo0J67/85S/1y1/+MtlNSQoGFQEAMHwwVxEAALANggsAALANggsAALANggsAALANgksfGEsEAMDwQXDpw76mDknS//PuoRS3BAAAEFz6qWrz3lQ3AQCAcx7BBQAA2AbBBQAA2AbBBQAA2AbBBQAA2AbBBQAA2AbBBQAA2AbBBQAA2AbBZQCWPbdD62o+TXUzAAA4ZxFcBuAP7x3SXWtqUt0MAADOWQQXAABgGwQXAABgGwQXAABgGwQXAABgGwQXAABgGwQXAABgGwQXAABgGwQXAABgGwQXAABgGwQXAABgGwQXAABgGwSXPvzbFz+X6iYAAIATCC59yMn0pLoJAADgBIJLH1xOK9VNAAAAJxBc+uCwCC4AAAwXBJc+OB0EFwAAhguCSx8ILgAADB8Elz44uVQEAMCwQXDpg9dNFwEAMFzwqdyH7HSGQwMAMFwQXPqQneFOdRMAAMAJBJc+FPnTlO52JmwLRWIpag0AAOc2gksfXE6Hrr84N2Hb797ep8OBLu0/1qFYzKSoZQAAnHssY8ygfPKuWrVKjzzyiBoaGjR9+nQ9/vjjuuqqq3ot/+KLL+qBBx7Q3r17NXHiRP3sZz/TF7/4xX49ViAQkN/vV0tLi3w+X7KeQpwxRt99/h298t6hXstML8nWrxZdrnf3N+vtT5pU6E+X22kp0BVRpsepqs179ZN5U/T5CblqD0Z0tC2ksedlKM3t1LH2kHxpLrmcf8uRsZiR45Sh2LGYkWVJFqOcgGHvYHOnsjPcyvC4Ut0UYFg7m8/vQQkuL7zwghYvXqwnn3xSpaWleuyxx/Tiiy9q165dys/PP6385s2bdf3116uyslJf/vKX9dxzz+lnP/uZduzYoSlTpvT5eIMdXE7a1dCq2Y9tGrTj98TttBSO9vwnGuV1qS0Y6fex5l92vixLemnHpxrldemivEz50t3yuhx67aPDkqRZk/NV/fFhFfnTdKilS+Wfy9enzV26YuxobfmkScFIVB6nQ5GY0b6mjvix/89ZE5U7yiOHZemNjw/rWEdIGR6nHFZ3eBufm6mrx+fof949qBsuztPHDa2qPdym9w606J9Kx2h8bqaOtoXUFY7qovxRcjssORyWstPdagh06WhbSFOKfQpFYzrY3Kmx52XKl+ZWNGZU19Quj9NSYyConEyPJhVmqSMUldOy1BmOqiQnXYeau9QVjsrrduiNj4/ohovzlOF1qrkjrH1NHZqQP0rNHSEFuiLK8Dg1NidDF4zOUEc4IkuWjrWH5HE5tGPfcRX605SV5pJlWTov0yN/ulsHjnfq4oJROtIW1LG2kNI9TmVneBSOxpSd4dYor0vHO8Jq6QgrJ9OjtmBEliUZI2WluRSKxNTcEdb4vEw5HZaCkZjC0ZgaWro0yuuK/50yPE6Fot2XKrvCMYUiMXndDrV1ReRyWBqV5lKGxyVjjAJdEfnSXOoMR+V0WDreHpbH5UgIybGYUXNnWNGYUYbHKcvq/rVoj9Mho9N/x6gtGFG626nmjpBGZ3ji4doYo65wTGluhyzLUjASlcvhUDgaU0coqux0d0IQ740xJh7OYzGjznBUmV5Xwva+6p76ltbfOp/9d28++4WiJ/uPdei6n7+hKef79Mp3rztjWZzbAl1hNbWFNC43M9VNSZlhE1xKS0t15ZVX6oknnpAkxWIxlZSU6Lvf/a5+8IMfnFZ+wYIFam9v1yuvvBLfdvXVV2vGjBl68skn+3y8oQouknTgeIf+9EGjtu07plffbxjUxwLOhsthKZKES5hup6W8UV4dbOmSx+U47d4uj9OhUWkuxYxRc0c4vt1hST09fIHPq+aOsJwOS5GoUcx0Lxkel3xpLrV0hhWOGvnS3YrEYuoIRuMhTZLG5GQoFInJYUldJ4JdKBKT1+VQoKs7wHtcDsViRpGYkcOS8rPS5HZZMqY7JAY6w8r3eRWJmfiZT6/LoeCJ5+ZPdyvN7VCm16WWjnB3uG0PKxiJKXqivcX+dAW6wmrtimhyYZaM6X5faA9FdV6mR03toXibp57vl8Nh6WhrUJ82dyon06MMj1PBSEwep0MOh+R2OORydvdJdoZbjYGgjrR2h3B/ulvNnSH5092qO9quIn+6sjPcCkVi3X1jpEyvSw5LkmUp3e1Qc0dYnx7vlNvl0EV5mQpHjdwn5lyrO9qhdI9DuaO8isWMOkJRpXuc3cc7cczzMj3K8HT/XY2RIrGYWrsiihqj4+0hFfrT5UtzyWFZ6ghF9Glzp0pyMuR2OBQzRlFj5LAs+dJcOtwalMOylJ3hVkcoqpjp7vdj7WEFI1FdWuzT6AyPAl1htXVF5HU55XJasizp40Otysvy6oLR6YqZ7nsLozEjI6POUFQup0PuE/2WldZ9Zutk8GzrisjpsBQ1Rl6XQ5GoOXFcSy6H1d3OEy/S/609qpKcDBX70xWOxhQzRllp3a/BYDimznBUMSM5HVJOplc5GW61BSMKR43S3A5FY0afHGnXBTkZisVM9+sy2v36HJuTqUMtncpKc8uyFG/LSdUfd39hdDksXTMhV05L6ghF5U93KxIz3fdXnpKTPxuZTw3aVsL2z5Trpc5n90WNUaAzrAyPS6FoTMFITC5Hd5+5XQ4ZY/SrRTOVTMMiuIRCIWVkZOi///u/NW/evPj2JUuWqLm5WevWrTutzpgxY7R8+XLdfffd8W0rV67U2rVr9e67755WPhgMKhgMxtcDgYBKSkqGJLj0xhijpvaQstJc6ghGtf94h7LTPYqZ7jcjS5b2HG3TweZOvbu/WQ2BoKYU+9QZjqrYn648n1dbP2nS77bWnzjF7NSYnExNLszS9n3HNakwS4X+NG3fd1wfHQrowPFOzb/sfL32YaMmFozSZWNG6+V3PlVzR+i0Dw3Lknxpbs2dUaz9xzr0173H1RaM6KZJeTraFtKx9pA+be6UJF0+Jls76puV5naoKxzr/s/fjw/B8zI9Gp3pUVNbUC6nQ0dagxqXm6m6o+2nlS30pakh0JWwbUL+KNUebpMkZXldaj1xJumSIp8aA10aleZSpselDw8FJHV/aI45L0ONgS4FT7zpStL52enx53JSVppLvjT3aduHgtflUCga0+BckAWAoeN0WPrgR7OV9pkBK3+PswkuSb8Ae/ToUUWjURUUFCRsLygo0Mcff9xjnYaGhh7LNzT0fEajsrJSP/rRj5LT4CSxLEu5o7ySJK/LqdGZp//+y+VjRuvyMaP15WnFPR7jpkn5WlHxubNuwwNfvuSs645UvZ3+P7k9euKb+allTn7rcjm6zzKcvPxxar1QJNZ9D1NnRGkeR/yyiNQdVrq/oXXXaQ9GFDlxKaYrHO3+VnniUlg4GpPjxOWNSMwoGI7J6ez+hnM4EFS+z6vYidRzpDUor8spr8sht8sRP7MRicbi31TbuiLxy0w5mR61dkW6z0IYozS3U9Go0f7jHTJGcrssjc7wyOmwlOZ26mhrUP50t9wuh461hdQWjCgYiaqpLaRCf5qiMaOcTI88LodGeV061h7S0bagMjwupbud8rod+uRIu/KyPHI5HHI6LH3a3KmWzrCy07vv93Ce+LY7yuuSkdQVjqojFFUwHFVRdnq8r9zO7r+N29n9jfb/a2xVVppbbqcll8OhfJ9XLoelUDSmQGdEkWhMB453qsCf1n0mw5LcLofcDofCsZi6T0hYisa6L12lu53yuLrPTgS6wspKcysaiyk/K00NLV0yJ/7W/nS3jneE1RWOqsifpo5QVKPSXKo93CZjuv/WBb40Bbq6zzhlZ7jlsCxlnjhj0dQeVCwmBSMxfXKkTZOLfPKludR14ozLp82dOtYe1MT8rPhZn6NtQRX505Q7yqtAV1jhaEyfHGmX1+1Udnp3H3hdTrUFI3JY1okzOFF1hqNqD0aVO8qrfU3tyvd55XF2P89orPvb854j7Tov0yO306GcTI/agxG5nJZ8aW4ZdV/+jRnFyzgsKRIz2r7vuErH5ai5M6z8LK9au7ovdcaMUWeo+3JoOBqT2+lQMBKVw7LU2hVRWzCinExP9+vdsuRxOU6cWYupqa378mtWmksuh0PNnSGNzclUJNbdD4cDQXndDrlPXJr2nbg8GzrlbNvoTI8cJ85SWOo+qxYzRl3hqDK8ru7XSCSmdI9T0ZjR8Y6w3E7rxGux+yzO+5+2yJ/uVqE/XZFoTC6nQy0dIeVnpSkc6z7j0BbsPgsSixkd7wgpK80tj8uhrnBUXeGowlGj3FHdl4DbgxE1d4SVneFWmtupw61B5Y7yKCfTq45QRF6XQ8Z0vybaQxHVHWnXtJJsyRgFIzEdbg2qZHR6/P3p1Pes+L8T3svUy/aevy0lljcJ2x2WpUyvS6FIVB6X88TZs+5LpO2hqMbnZp52NicVbHnn2IoVK7R8+fL4+skzLsBn9XbPwsntPc1F5T7lJul0T+I3i5P1PK7uMv5TfufH6fhbWecph830/u2/2anHTly35HIq4ZvMmPMyEsqOPa/v/66fDcynPvZJ/gx/j3VHnVJ2VA/1PivT61JJTmIbC3xpCeuf3X+2ppzfc5sHQ38e68oLc4agJX/zf0we0oc7zS3Te/6yNRIsuDLVLcBAJT245Obmyul0qrGxMWF7Y2OjCgsLe6xTWFg4oPJer1derzc5DQYAALaR9N9x8Xg8mjlzpqqrq+PbYrGYqqurVVZW1mOdsrKyhPKStGHDhl7LAwCAc9OgXCpavny5lixZoiuuuEJXXXWVHnvsMbW3t+v222+XJC1evFjnn3++KisrJUl33XWXbrjhBv3iF7/Ql770Ja1Zs0bbtm3Tf/7nfw5G8wAAgE0NSnBZsGCBjhw5ogcffFANDQ2aMWOG1q9fH78Bt76+Xg7H3072XHPNNXruued0//3364c//KEmTpyotWvX9us3XAAAwLlj0H45dygN5e+4AACA5Dibz2/mKgIAALZBcAEAALZBcAEAALZBcAEAALZBcAEAALZBcAEAALZBcAEAALZBcAEAALZhy9mhP+vkb+gFAoEUtwQAAPTXyc/tgfwW7ogILq2trZKkkpKSFLcEAAAMVGtrq/x+f7/Kjoif/I/FYjp48KCysrJkWVZSjx0IBFRSUqL9+/czncAQoL+HDn09tOjvoUV/D52/p6+NMWptbVVxcXHCHIZnMiLOuDgcDl1wwQWD+hg+n48X/xCiv4cOfT206O+hRX8PnbPt6/6eaTmJm3MBAIBtEFwAAIBtEFz64PV6tXLlSnm93lQ35ZxAfw8d+npo0d9Di/4eOkPd1yPi5lwAAHBu4IwLAACwDYILAACwDYILAACwDYILAACwDYJLH1atWqULL7xQaWlpKi0t1V/+8pdUN8l2HnroIVmWlbBMnjw5vr+rq0vLli3Teeedp1GjRulrX/uaGhsbE45RX1+vL33pS8rIyFB+fr7uvfdeRSKRoX4qw86mTZt0yy23qLi4WJZlae3atQn7jTF68MEHVVRUpPT0dJWXl2v37t0JZY4dO6ZFixbJ5/MpOztb3/zmN9XW1pZQ5r333tN1112ntLQ0lZSU6Oc///lgP7Vhqa/+vu222057rc+ZMyehDP3dP5WVlbryyiuVlZWl/Px8zZs3T7t27Uook6z3jo0bN+ryyy+X1+vVhAkTVFVVNdhPb9jpT3/feOONp72+v/Od7ySUGZL+NujVmjVrjMfjMU899ZT54IMPzNKlS012drZpbGxMddNsZeXKlebSSy81hw4dii9HjhyJ7//Od75jSkpKTHV1tdm2bZu5+uqrzTXXXBPfH4lEzJQpU0x5ebl55513zKuvvmpyc3PNihUrUvF0hpVXX33V/Nu//Zt56aWXjCTz8ssvJ+z/6U9/avx+v1m7dq159913zVe+8hUzbtw409nZGS8zZ84cM336dLN161bz5z//2UyYMMEsXLgwvr+lpcUUFBSYRYsWmZ07d5rnn3/epKenm1//+tdD9TSHjb76e8mSJWbOnDkJr/Vjx44llKG/+2f27Nnm6aefNjt37jQ1NTXmi1/8ohkzZoxpa2uLl0nGe8cnn3xiMjIyzPLly82HH35oHn/8ceN0Os369euH9PmmWn/6+4YbbjBLly5NeH23tLTE9w9VfxNczuCqq64yy5Yti69Ho1FTXFxsKisrU9gq+1m5cqWZPn16j/uam5uN2+02L774YnzbRx99ZCSZLVu2GGO6PywcDodpaGiIl1m9erXx+XwmGAwOatvt5LMfpLFYzBQWFppHHnkkvq25udl4vV7z/PPPG2OM+fDDD40k89e//jVe5o9//KOxLMt8+umnxhhjfvWrX5nRo0cn9PV9991nJk2aNMjPaHjrLbjMnTu31zr099k7fPiwkWTefPNNY0zy3ju+//3vm0svvTThsRYsWGBmz5492E9pWPtsfxvTHVzuuuuuXusMVX9zqagXoVBI27dvV3l5eXybw+FQeXm5tmzZksKW2dPu3btVXFys8ePHa9GiRaqvr5ckbd++XeFwOKGfJ0+erDFjxsT7ecuWLZo6daoKCgriZWbPnq1AIKAPPvhgaJ+IjdTV1amhoSGhb/1+v0pLSxP6Njs7W1dccUW8THl5uRwOh95+++14meuvv14ejydeZvbs2dq1a5eOHz8+RM/GPjZu3Kj8/HxNmjRJd9xxh5qamuL76O+z19LSIknKycmRlLz3ji1btiQc42SZc/19/rP9fdLvfvc75ebmasqUKVqxYoU6Ojri+4aqv0fEJIuD4ejRo4pGowl/AEkqKCjQxx9/nKJW2VNpaamqqqo0adIkHTp0SD/60Y903XXXaefOnWpoaJDH41F2dnZCnYKCAjU0NEiSGhoaevw7nNyHnp3sm5767tS+zc/PT9jvcrmUk5OTUGbcuHGnHePkvtGjRw9K++1ozpw5+upXv6px48Zpz549+uEPf6iKigpt2bJFTqeT/j5LsVhMd999tz7/+c9rypQpkpS0947eygQCAXV2dio9PX0wntKw1lN/S9I//dM/aezYsSouLtZ7772n++67T7t27dJLL70kaej6m+CCQVdRURH/97Rp01RaWqqxY8fq97///Tn5poCR6+tf/3r831OnTtW0adN00UUXaePGjZo1a1YKW2Zvy5Yt086dO/XWW2+luinnhN76+9vf/nb831OnTlVRUZFmzZqlPXv26KKLLhqy9nGpqBe5ublyOp2n3aHe2NiowsLCFLVqZMjOztbFF1+s2tpaFRYWKhQKqbm5OaHMqf1cWFjY49/h5D707GTfnOk1XFhYqMOHDyfsj0QiOnbsGP2fBOPHj1dubq5qa2sl0d9n484779Qrr7yiN954QxdccEF8e7LeO3or4/P5zskvVr31d09KS0slKeH1PRT9TXDphcfj0cyZM1VdXR3fFovFVF1drbKyshS2zP7a2tq0Z88eFRUVaebMmXK73Qn9vGvXLtXX18f7uaysTO+//37CG/6GDRvk8/l0ySWXDHn77WLcuHEqLCxM6NtAIKC33347oW+bm5u1ffv2eJnXX39dsVgs/qZUVlamTZs2KRwOx8ts2LBBkyZNOicvWwzEgQMH1NTUpKKiIkn090AYY3TnnXfq5Zdf1uuvv37a5bNkvXeUlZUlHONkmXPtfb6v/u5JTU2NJCW8voekv/t9G+85aM2aNcbr9Zqqqirz4Ycfmm9/+9smOzs74Y5p9O173/ue2bhxo6mrqzP/+7//a8rLy01ubq45fPiwMaZ7SOOYMWPM66+/brZt22bKyspMWVlZvP7JIXY333yzqampMevXrzd5eXkMhzbGtLa2mnfeece88847RpJ59NFHzTvvvGP27dtnjOkeDp2dnW3WrVtn3nvvPTN37tweh0Nfdtll5u233zZvvfWWmThxYsLw3ObmZlNQUGD+5V/+xezcudOsWbPGZGRknHPDc405c3+3traae+65x2zZssXU1dWZ1157zVx++eVm4sSJpqurK34M+rt/7rjjDuP3+83GjRsTht92dHTEyyTjvePk8Nx7773XfPTRR2bVqlXn5HDovvq7trbW/PjHPzbbtm0zdXV1Zt26dWb8+PHm+uuvjx9jqPqb4NKHxx9/3IwZM8Z4PB5z1VVXma1bt6a6SbazYMECU1RUZDwejzn//PPNggULTG1tbXx/Z2en+dd//VczevRok5GRYebPn28OHTqUcIy9e/eaiooKk56ebnJzc833vvc9Ew6Hh/qpDDtvvPGGkXTasmTJEmNM95DoBx54wBQUFBiv12tmzZpldu3alXCMpqYms3DhQjNq1Cjj8/nM7bffblpbWxPKvPvuu+baa681Xq/XnH/++eanP/3pUD3FYeVM/d3R0WFuvvlmk5eXZ9xutxk7dqxZunTpaV906O/+6amfJZmnn346XiZZ7x1vvPGGmTFjhvF4PGb8+PEJj3Gu6Ku/6+vrzfXXX29ycnKM1+s1EyZMMPfee2/C77gYMzT9bZ1oMAAAwLDHPS4AAMA2CC4AAMA2CC4AAMA2CC4AAMA2CC4AAMA2CC4AAMA2CC4AAMA2CC4AAMA2CC4AAMA2CC4AAMA2CC4AAMA2CC4AAMA2/n8rNTR0xNeTUAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2392b914df0>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE=32\n",
    "learning_rate=0.001\n",
    "hidden_size=64\n",
    "embed_dim=64\n",
    "brands_dim=brand.max()+1\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "network=Model(X_train.shape[1],brands_dim,embed_dim,hidden_size).to(device)\n",
    "optimizer = torch.optim.Adam(network.parameters(),lr=learning_rate)\n",
    "loss = torch.nn.functional.mse_loss\n",
    "train_res,test_res,test_r_sq,train_r_sq = train(train_dataloader,network,loss,optimizer,epochs=50,test_dataloader=test_dataloader)\n",
    "\n",
    "result = torch.tensor(train_res).flatten()\n",
    "elements = result.shape[0]\n",
    "window_size = 50\n",
    "to_plot=result[-window_size*(elements//window_size):].view(-1,window_size).mean(-1)\n",
    "plt.plot(to_plot)\n",
    "plt.show()\n",
    "plt.plot(test_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
